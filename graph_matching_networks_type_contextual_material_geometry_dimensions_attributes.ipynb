{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HrOZEU_OrYTR"
      },
      "source": [
        "##### Copyright 2019 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G9suJlqLrfmP"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modifications copyright (c) 2022 Dan Brennan/University of Sheffield\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F50gn0G_yZ3s"
      },
      "source": [
        "# Graph Matching Networks for Learning the Similarity of Graph Structured Objects\n",
        "\n",
        "This is the example code for our ICML 2019 paper.  Please refer to the paper for more details:\n",
        "\n",
        "> Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli.  *Graph Matching Networks for Learning the Similarity of Graph Structured Objects*.  ICML 2019.  [\\[arXiv\\]](https://arxiv.org/abs/1904.12787)\n",
        "\n",
        "Modifications have been made for the ISMA 2022 paper to load in IE models into the GMN and compute a heatmap of similarity. The paper details are:\n",
        "\n",
        "> Daniel S. Brennan, Timothy J. Rogers, Elizabeth J. Cross, Keith Worden.  * Calculating structure similarity via a Graph Neural Network in Population-based Structural Health Monitoring* ISMA 2022.\n",
        "\n",
        "The GMN code was taken from https://github.com/deepmind/deepmind-research/tree/master/graph_matching_networks and has then been modified to add in AttributedGraphDataset class and associated config modifications as well as generating heatmap of the results.\n",
        "\n",
        "Any modification to either the markdown or the code will for this point onwards be highlighted by *#DSB Modification*, otherwise this should be considered as being written by Li et al.\n",
        "\n",
        "## Graph similarity learning\n",
        "\n",
        "Our goal is to learn a similarity function between graphs.  Given two graphs $G_1, G_2$, a graph similarity model can be written as a function $f(G_1, G_2)$ that computes a scalar similarity value.\n",
        "\n",
        "In this project we build models to learn such a similarity function based on examples of similar / dissimilar pairs or triplets.  Because of learning, our model can adapt to different notions of similarity and to different types of graph structure, as long as training data is available.\n",
        "\n",
        "In the following we will sometimes use the term \"distance\" and say the model learns a \"distance function\" $d(G_1, G_2)$ between graphs when convenient.  But this is just the opposite of a similarity function, and you may simply say $f(G_1, G_2) = - d(G_1, G_2)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4QqVQJxLoYKx"
      },
      "source": [
        "## Some dependencies and imports\n",
        "\n",
        "If you want to run the notebook locally, make sure you have all the dependencies first.  You can use the following command\n",
        "```\n",
        "pip3 install --user -r requirements.txt\n",
        "```\n",
        "\n",
        "Note the code should work for both python 3 and 2, but python 3 is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-eanBQF6boRu"
      },
      "outputs": [],
      "source": [
        "# Let's disable all the warnings first\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dyu1M3fufCaD"
      },
      "source": [
        "These are all the dependencies that will be used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0Ep7LRdJoZ6O"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import collections\n",
        "import contextlib\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import six\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "#DSB Modification: Add in the included dependencies required for our modifications\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fzeL1Fk8zD3S"
      },
      "source": [
        "## The models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gSYjjfgvffJO"
      },
      "source": [
        "### The graph embedding model\n",
        "\n",
        "The simpler variant of our model is based on embedding each graph **independently** into a vector and then use an existing distance (or similarity) metric in the vector space to compute the distance between graphs.  More concretely, we define\n",
        "\n",
        "$$d(G_1, G_2) = d_H(embed(G_1), embed(G_2)),$$\n",
        "\n",
        "where $embed$ is a model that maps any graph $G$ into an $H$-dimensional vector, and $d_H$ is a distance metric in that vector space.  Typical examples are Euclidean distance in $\\mathbb{R}^H$, i.e. $d_H(x, y) = \\sqrt{\\sum_{i=1}^H (x_i - y_i)^2}$, or Hamming distance in $H$-dimensional space of binary vectors, i.e. $d_H(x, y)=\\sum_{i=1}^H \\mathbb{I}[x_i \\ne y_i]$.\n",
        "\n",
        "Each graph input contains a set of nodes $V$ and edges $E$.  Each node $i\\in V$ may have a feature vector $x_i$ associated with it, and each edge $(i, j)\\in E$ may also have a feature vector $x_{ij}$ encoding e.g. edge type or attributes.  The embedding model will therefore jointly reason about the graph structure as well as the graph features to come up with an embedding that reflects the notion of similarity described by the training examples.\n",
        "\n",
        "The embedding model is composed of 3 parts:\n",
        "1. An encoder that maps $x_i$ and $x_{ij}$ into a nice hidden representation space.  Here we use separate MLPs (fully connected neural nets) for node and edge representations:\n",
        "$$\\begin{array}{rcl}\n",
        "h_i^{(0)} &=& \\mathrm{MLP_{node}}(x_i) \\\\\n",
        "e_{ij} &=& \\mathrm{MLP_{edge}}(x_{ij})\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "2. A graph neural network (GNN) that communicates information across the graph and computes node representations that encode local neighborhood structure and semantics.  More concretely, the GNN computes node representations through an iterative message passing process.  In the $t$-th round of message passing, we compute a message vector on each edge, and then each node aggregates all the incoming messages and updates its own representation:\n",
        "$$\\begin{array}{rcl}\n",
        "m_{i\\rightarrow j} &=& f_\\mathrm{message}(h_i^{(t)}, h_j^{(t)}, e_{ij}) \\\\\n",
        "h_i^{(t+1)} &=& f_\\mathrm{node}(h_i^{(t)}, \\sum_{j:(j,i)\\in E} m_{j\\rightarrow i})\n",
        "\\end{array}\n",
        "$$\n",
        "Here both $f_\\mathrm{message}$ and $f_\\mathrm{node}$ are neural modules.  We use MLPs for $f_\\mathrm{message}$, while $f_\\mathrm{node}$ can also be MLPs or even recurrent neural network cores like LSTMs or GRUs.  The GNNs have the nice property of being equivariant to node permutations, and nodes on isomorphic graphs (with the same node and edge features) will have the same representations regardless of the ordering.\n",
        "\n",
        "3. After we obtained the final node representations after $T$ rounds of message passing, we aggregate across them to get graph representations $h_G=f_G(\\{h_i^{(T)}\\}_{i\\in V})$.  This could be implemented by a simple sum that reduces the node representations into a single vector and then transform it:\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} h_i^{(T)}\\right).$$\n",
        "We used the following gated aggregation module proposed in [Li et al., 2015](https://arxiv.org/abs/1511.05493) which we found to work consistently better:\n",
        "$$h_G = \\mathrm{MLP_G}\\left(\\sum_{i\\in V} \\sigma(\\mathrm{MLP_{gate}}(h_i^{(T)})) \\odot \\mathrm{MLP}(h_i^{(T)})\\right).$$\n",
        "The key to this function is to make sure it is invariant to node orderings, both the above forms satisfy this condition.  The gated variant gives the model the capacity to explicitly modulate each node's contribution to the graph representation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8eVpPWEjZ_O"
      },
      "source": [
        "#### The graph encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "b8dFQUFwjcCi"
      },
      "outputs": [],
      "source": [
        "class GraphEncoder(snt.AbstractModule):\n",
        "  \"\"\"Encoder module that projects node and edge features to some embeddings.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_hidden_sizes=None,\n",
        "               edge_hidden_sizes=None,\n",
        "               name='graph-encoder'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_hidden_sizes: if provided should be a list of ints, hidden sizes of\n",
        "        node encoder network, the last element is the size of the node outputs.\n",
        "        If not provided, node features will pass through as is.\n",
        "      edge_hidden_sizes: if provided should be a list of ints, hidden sizes of\n",
        "        edge encoder network, the last element is the size of the edge outptus.\n",
        "        If not provided, edge features will pass through as is.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphEncoder, self).__init__(name=name)\n",
        "\n",
        "    # this also handles the case of an empty list\n",
        "    self._node_hidden_sizes = node_hidden_sizes if node_hidden_sizes else None\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes\n",
        "\n",
        "  def _build(self, node_features, edge_features=None):\n",
        "    \"\"\"Encode node and edge features.\n",
        "\n",
        "    Args:\n",
        "      node_features: [n_nodes, node_feat_dim] float tensor.\n",
        "      edge_features: if provided, should be [n_edges, edge_feat_dim] float\n",
        "        tensor.\n",
        "\n",
        "    Returns:\n",
        "      node_outputs: [n_nodes, node_embedding_dim] float tensor, node embeddings.\n",
        "      edge_outputs: if edge_features is not None and edge_hidden_sizes is not\n",
        "        None, this is [n_edges, edge_embedding_dim] float tensor, edge\n",
        "        embeddings; otherwise just the input edge_features.\n",
        "    \"\"\"\n",
        "    if self._node_hidden_sizes is None:\n",
        "      node_outputs = node_features\n",
        "    else:\n",
        "      node_outputs = snt.nets.MLP(\n",
        "          self._node_hidden_sizes, name='node-feature-mlp')(node_features)\n",
        "\n",
        "    if edge_features is None or self._edge_hidden_sizes is None:\n",
        "      edge_outputs = edge_features\n",
        "    else:\n",
        "      edge_outputs = snt.nets.MLP(\n",
        "          self._edge_hidden_sizes, name='edge-feature-mlp')(edge_features)\n",
        "\n",
        "    return node_outputs, edge_outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "goVx4QGClJCW"
      },
      "source": [
        "#### The message passing layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5LJIBWdvlIWl"
      },
      "outputs": [],
      "source": [
        "def graph_prop_once(node_states,\n",
        "                    from_idx,\n",
        "                    to_idx,\n",
        "                    message_net,\n",
        "                    aggregation_module=tf.unsorted_segment_sum,\n",
        "                    edge_features=None):\n",
        "  \"\"\"One round of propagation (message passing) in a graph.\n",
        "\n",
        "  Args:\n",
        "    node_states: [n_nodes, node_state_dim] float tensor, node state vectors, one\n",
        "      row for each node.\n",
        "    from_idx: [n_edges] int tensor, index of the from nodes.\n",
        "    to_idx: [n_edges] int tensor, index of the to nodes.\n",
        "    message_net: a network that maps concatenated edge inputs to message\n",
        "      vectors.\n",
        "    aggregation_module: a module that aggregates messages on edges to aggregated\n",
        "      messages for each node.  Should be a callable and can be called like the\n",
        "      following,\n",
        "      `aggregated_messages = aggregation_module(messages, to_idx, n_nodes)`,\n",
        "      where messages is [n_edges, edge_message_dim] tensor, to_idx is the index\n",
        "      of the to nodes, i.e. where each message should go to, and n_nodes is an\n",
        "      int which is the number of nodes to aggregate into.\n",
        "    edge_features: if provided, should be a [n_edges, edge_feature_dim] float\n",
        "      tensor, extra features for each edge.\n",
        "\n",
        "  Returns:\n",
        "    aggregated_messages: an [n_nodes, edge_message_dim] float tensor, the\n",
        "      aggregated messages, one row for each node.\n",
        "  \"\"\"\n",
        "  from_states = tf.gather(node_states, from_idx)\n",
        "  to_states = tf.gather(node_states, to_idx)\n",
        "\n",
        "  edge_inputs = [from_states, to_states]\n",
        "  if edge_features is not None:\n",
        "    edge_inputs.append(edge_features)\n",
        "\n",
        "  edge_inputs = tf.concat(edge_inputs, axis=-1)\n",
        "  messages = message_net(edge_inputs)\n",
        "\n",
        "  return aggregation_module(messages, to_idx, tf.shape(node_states)[0])\n",
        "\n",
        "\n",
        "class GraphPropLayer(snt.AbstractModule):\n",
        "  \"\"\"Implementation of a graph propagation (message passing) layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_state_dim,\n",
        "               edge_hidden_sizes,\n",
        "               node_hidden_sizes,\n",
        "               edge_net_init_scale=0.1,\n",
        "               node_update_type='residual',\n",
        "               use_reverse_direction=True,\n",
        "               reverse_dir_param_different=True,\n",
        "               layer_norm=False,\n",
        "               name='graph-net'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_state_dim: int, dimensionality of node states.\n",
        "      edge_hidden_sizes: list of ints, hidden sizes for the edge message\n",
        "        net, the last element in the list is the size of the message vectors.\n",
        "      node_hidden_sizes: list of ints, hidden sizes for the node update\n",
        "        net.\n",
        "      edge_net_init_scale: initialization scale for the edge networks.  This\n",
        "        is typically set to a small value such that the gradient does not blow\n",
        "        up.\n",
        "      node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "      use_reverse_direction: set to True to also propagate messages in the\n",
        "        reverse direction.\n",
        "      reverse_dir_param_different: set to True to have the messages computed\n",
        "        using a different set of parameters than for the forward direction.\n",
        "      layer_norm: set to True to use layer normalization in a few places.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphPropLayer, self).__init__(name=name)\n",
        "\n",
        "    self._node_state_dim = node_state_dim\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes[:]\n",
        "\n",
        "    # output size is node_state_dim\n",
        "    self._node_hidden_sizes = node_hidden_sizes[:] + [node_state_dim]\n",
        "    self._edge_net_init_scale = edge_net_init_scale\n",
        "    self._node_update_type = node_update_type\n",
        "\n",
        "    self._use_reverse_direction = use_reverse_direction\n",
        "    self._reverse_dir_param_different = reverse_dir_param_different\n",
        "\n",
        "    self._layer_norm = layer_norm\n",
        "\n",
        "  def _compute_aggregated_messages(\n",
        "      self, node_states, from_idx, to_idx, edge_features=None):\n",
        "    \"\"\"Compute aggregated messages for each node.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "      from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "      to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "      edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "        tensor, edge features.\n",
        "\n",
        "    Returns:\n",
        "      aggregated_messages: [n_nodes, aggregated_message_dim] float tensor, the\n",
        "        aggregated messages for each node.\n",
        "    \"\"\"\n",
        "    self._message_net = snt.nets.MLP(\n",
        "        self._edge_hidden_sizes,\n",
        "        initializers={\n",
        "            'w': tf.variance_scaling_initializer(\n",
        "                scale=self._edge_net_init_scale),\n",
        "            'b': tf.zeros_initializer()},\n",
        "        name='message-mlp')\n",
        "\n",
        "    aggregated_messages = graph_prop_once(\n",
        "        node_states,\n",
        "        from_idx,\n",
        "        to_idx,\n",
        "        self._message_net,\n",
        "        aggregation_module=tf.unsorted_segment_sum,\n",
        "        edge_features=edge_features)\n",
        "\n",
        "    # optionally compute message vectors in the reverse direction\n",
        "    if self._use_reverse_direction:\n",
        "      if self._reverse_dir_param_different:\n",
        "        self._reverse_message_net = snt.nets.MLP(\n",
        "            self._edge_hidden_sizes,\n",
        "            initializers={\n",
        "                'w': tf.variance_scaling_initializer(\n",
        "                    scale=self._edge_net_init_scale),\n",
        "                'b': tf.zeros_initializer()},\n",
        "            name='reverse-message-mlp')\n",
        "      else:\n",
        "        self._reverse_message_net = self._message_net\n",
        "\n",
        "      reverse_aggregated_messages = graph_prop_once(\n",
        "          node_states,\n",
        "          to_idx,\n",
        "          from_idx,\n",
        "          self._reverse_message_net,\n",
        "          aggregation_module=tf.unsorted_segment_sum,\n",
        "          edge_features=edge_features)\n",
        "\n",
        "      aggregated_messages += reverse_aggregated_messages\n",
        "\n",
        "    if self._layer_norm:\n",
        "      aggregated_messages = snt.LayerNorm()(aggregated_messages)\n",
        "\n",
        "    return aggregated_messages\n",
        "\n",
        "  def _compute_node_update(self,\n",
        "                           node_states,\n",
        "                           node_state_inputs,\n",
        "                           node_features=None):\n",
        "    \"\"\"Compute node updates.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, the input node\n",
        "        states.\n",
        "      node_state_inputs: a list of tensors used to compute node updates.  Each\n",
        "        element tensor should have shape [n_nodes, feat_dim], where feat_dim can\n",
        "        be different.  These tensors will be concatenated along the feature\n",
        "        dimension.\n",
        "      node_features: extra node features if provided, should be of size\n",
        "        [n_nodes, extra_node_feat_dim] float tensor, can be used to implement\n",
        "        different types of skip connections.\n",
        "\n",
        "    Returns:\n",
        "      new_node_states: [n_nodes, node_state_dim] float tensor, the new node\n",
        "        state tensor.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if node update type is not supported.\n",
        "    \"\"\"\n",
        "    if self._node_update_type in ('mlp', 'residual'):\n",
        "      node_state_inputs.append(node_states)\n",
        "    if node_features is not None:\n",
        "      node_state_inputs.append(node_features)\n",
        "\n",
        "    if len(node_state_inputs) == 1:\n",
        "      node_state_inputs = node_state_inputs[0]\n",
        "    else:\n",
        "      node_state_inputs = tf.concat(node_state_inputs, axis=-1)\n",
        "\n",
        "    if self._node_update_type == 'gru':\n",
        "      _, new_node_states = snt.GRU(self._node_state_dim)(\n",
        "          node_state_inputs, node_states)\n",
        "      return new_node_states\n",
        "    else:\n",
        "      mlp_output = snt.nets.MLP(\n",
        "          self._node_hidden_sizes, name='node-mlp')(node_state_inputs)\n",
        "      if self._layer_norm:\n",
        "        mlp_output = snt.LayerNorm()(mlp_output)\n",
        "      if self._node_update_type == 'mlp':\n",
        "        return mlp_output\n",
        "      elif self._node_update_type == 'residual':\n",
        "        return node_states + mlp_output\n",
        "      else:\n",
        "        raise ValueError('Unknown node update type %s' % self._node_update_type)\n",
        "\n",
        "  def _build(self,\n",
        "             node_states,\n",
        "             from_idx,\n",
        "             to_idx,\n",
        "             edge_features=None,\n",
        "             node_features=None):\n",
        "    \"\"\"Run one propagation step.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "      from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "      to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "      edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "        tensor, edge features.\n",
        "      node_features: extra node features if provided, should be of size\n",
        "        [n_nodes, extra_node_feat_dim] float tensor, can be used to implement\n",
        "        different types of skip connections.\n",
        "\n",
        "    Returns:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, new node states.\n",
        "    \"\"\"\n",
        "    aggregated_messages = self._compute_aggregated_messages(\n",
        "        node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "    return self._compute_node_update(node_states,\n",
        "                                     [aggregated_messages],\n",
        "                                     node_features=node_features)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubKr-G_Sq95j"
      },
      "source": [
        "#### Graph aggregator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NmE-8Xpnq9Zi"
      },
      "outputs": [],
      "source": [
        "AGGREGATION_TYPE = {\n",
        "    'sum': tf.unsorted_segment_sum,\n",
        "    'mean': tf.unsorted_segment_mean,\n",
        "    'sqrt_n': tf.unsorted_segment_sqrt_n,\n",
        "    'max': tf.unsorted_segment_max,\n",
        "}\n",
        "\n",
        "\n",
        "class GraphAggregator(snt.AbstractModule):\n",
        "  \"\"\"This module computes graph representations by aggregating from parts.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_hidden_sizes,\n",
        "               graph_transform_sizes=None,\n",
        "               gated=True,\n",
        "               aggregation_type='sum',\n",
        "               name='graph-aggregator'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_hidden_sizes: the hidden layer sizes of the node transformation nets.\n",
        "        The last element is the size of the aggregated graph representation.\n",
        "      graph_transform_sizes: sizes of the transformation layers on top of the\n",
        "        graph representations.  The last element of this list is the final\n",
        "        dimensionality of the output graph representations.\n",
        "      gated: set to True to do gated aggregation, False not to.\n",
        "      aggregation_type: one of {sum, max, mean, sqrt_n}.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphAggregator, self).__init__(name=name)\n",
        "\n",
        "    self._node_hidden_sizes = node_hidden_sizes\n",
        "    self._graph_transform_sizes = graph_transform_sizes\n",
        "    self._graph_state_dim = node_hidden_sizes[-1]\n",
        "    self._gated = gated\n",
        "    self._aggregation_type = aggregation_type\n",
        "    self._aggregation_op = AGGREGATION_TYPE[aggregation_type]\n",
        "\n",
        "  def _build(self, node_states, graph_idx, n_graphs):\n",
        "    \"\"\"Compute aggregated graph representations.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, node states of a\n",
        "        batch of graphs concatenated together along the first dimension.\n",
        "      graph_idx: [n_nodes] int tensor, graph ID for each node.\n",
        "      n_graphs: integer, number of graphs in this batch.\n",
        "\n",
        "    Returns:\n",
        "      graph_states: [n_graphs, graph_state_dim] float tensor, graph\n",
        "        representations, one row for each graph.\n",
        "    \"\"\"\n",
        "    node_hidden_sizes = self._node_hidden_sizes\n",
        "    if self._gated:\n",
        "      node_hidden_sizes[-1] = self._graph_state_dim * 2\n",
        "\n",
        "    node_states_g = snt.nets.MLP(\n",
        "        node_hidden_sizes, name='node-state-g-mlp')(node_states)\n",
        "\n",
        "    if self._gated:\n",
        "      gates = tf.nn.sigmoid(node_states_g[:, :self._graph_state_dim])\n",
        "      node_states_g = node_states_g[:, self._graph_state_dim:] * gates\n",
        "\n",
        "    graph_states = self._aggregation_op(node_states_g, graph_idx, n_graphs)\n",
        "\n",
        "    # unsorted_segment_max does not handle empty graphs in the way we want\n",
        "    # it assigns the lowest possible float to empty segments, we want to reset\n",
        "    # them to zero.\n",
        "    if self._aggregation_type == 'max':\n",
        "      # reset everything that's smaller than -1e5 to 0.\n",
        "      graph_states *= tf.cast(graph_states > -1e5, tf.float32)\n",
        "\n",
        "    # transform the reduced graph states further\n",
        "\n",
        "    # pylint: disable=g-explicit-length-test\n",
        "    if (self._graph_transform_sizes is not None and\n",
        "        len(self._graph_transform_sizes) > 0):\n",
        "      graph_states = snt.nets.MLP(\n",
        "          self._graph_transform_sizes, name='graph-transform-mlp')(graph_states)\n",
        "\n",
        "    return graph_states"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vYMDsa4nmg-C"
      },
      "source": [
        "#### Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M7MvynyjmkVq"
      },
      "outputs": [],
      "source": [
        "class GraphEmbeddingNet(snt.AbstractModule):\n",
        "  \"\"\"A graph to embedding mapping network.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               encoder,\n",
        "               aggregator,\n",
        "               node_state_dim,\n",
        "               edge_hidden_sizes,\n",
        "               node_hidden_sizes,\n",
        "               n_prop_layers,\n",
        "               share_prop_params=False,\n",
        "               edge_net_init_scale=0.1,\n",
        "               node_update_type='residual',\n",
        "               use_reverse_direction=True,\n",
        "               reverse_dir_param_different=True,\n",
        "               layer_norm=False,\n",
        "               name='graph-embedding-net'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      encoder: GraphEncoder, encoder that maps features to embeddings.\n",
        "      aggregator: GraphAggregator, aggregator that produces graph\n",
        "        representations.\n",
        "      node_state_dim: dimensionality of node states.\n",
        "      edge_hidden_sizes: sizes of the hidden layers of the edge message nets.\n",
        "      node_hidden_sizes: sizes of the hidden layers of the node update nets.\n",
        "      n_prop_layers: number of graph propagation layers.\n",
        "      share_prop_params: set to True to share propagation parameters across all\n",
        "        graph propagation layers, False not to.\n",
        "      edge_net_init_scale: scale of initialization for the edge message nets.\n",
        "      node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "      use_reverse_direction: set to True to also propagate messages in the\n",
        "        reverse direction.\n",
        "      reverse_dir_param_different: set to True to have the messages computed\n",
        "        using a different set of parameters than for the forward direction.\n",
        "      layer_norm: set to True to use layer normalization in a few places.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphEmbeddingNet, self).__init__(name=name)\n",
        "\n",
        "    self._encoder = encoder\n",
        "    self._aggregator = aggregator\n",
        "    self._node_state_dim = node_state_dim\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes\n",
        "    self._node_hidden_sizes = node_hidden_sizes\n",
        "    self._n_prop_layers = n_prop_layers\n",
        "    self._share_prop_params = share_prop_params\n",
        "    self._edge_net_init_scale = edge_net_init_scale\n",
        "    self._node_update_type = node_update_type\n",
        "    self._use_reverse_direction = use_reverse_direction\n",
        "    self._reverse_dir_param_different = reverse_dir_param_different\n",
        "    self._layer_norm = layer_norm\n",
        "\n",
        "    self._prop_layers = []\n",
        "    self._layer_class = GraphPropLayer\n",
        "\n",
        "  def _build_layer(self, layer_id):\n",
        "    \"\"\"Build one layer in the network.\"\"\"\n",
        "    return self._layer_class(\n",
        "        self._node_state_dim,\n",
        "        self._edge_hidden_sizes,\n",
        "        self._node_hidden_sizes,\n",
        "        edge_net_init_scale=self._edge_net_init_scale,\n",
        "        node_update_type=self._node_update_type,\n",
        "        use_reverse_direction=self._use_reverse_direction,\n",
        "        reverse_dir_param_different=self._reverse_dir_param_different,\n",
        "        layer_norm=self._layer_norm,\n",
        "        name='graph-prop-%d' % layer_id)\n",
        "\n",
        "  def _apply_layer(self,\n",
        "                   layer,\n",
        "                   node_states,\n",
        "                   from_idx,\n",
        "                   to_idx,\n",
        "                   graph_idx,\n",
        "                   n_graphs,\n",
        "                   edge_features):\n",
        "    \"\"\"Apply one layer on the given inputs.\"\"\"\n",
        "    del graph_idx, n_graphs\n",
        "    return layer(node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "  def _build(self,\n",
        "             node_features,\n",
        "             edge_features,\n",
        "             from_idx,\n",
        "             to_idx,\n",
        "             graph_idx,\n",
        "             n_graphs):\n",
        "    \"\"\"Compute graph representations.\n",
        "\n",
        "    Args:\n",
        "      node_features: [n_nodes, node_feat_dim] float tensor.\n",
        "      edge_features: [n_edges, edge_feat_dim] float tensor.\n",
        "      from_idx: [n_edges] int tensor, index of the from node for each edge.\n",
        "      to_idx: [n_edges] int tensor, index of the to node for each edge.\n",
        "      graph_idx: [n_nodes] int tensor, graph id for each node.\n",
        "      n_graphs: int, number of graphs in the batch.\n",
        "\n",
        "    Returns:\n",
        "      graph_representations: [n_graphs, graph_representation_dim] float tensor,\n",
        "        graph representations.\n",
        "    \"\"\"\n",
        "    if len(self._prop_layers) < self._n_prop_layers:\n",
        "      # build the layers\n",
        "      for i in range(self._n_prop_layers):\n",
        "        if i == 0 or not self._share_prop_params:\n",
        "          layer = self._build_layer(i)\n",
        "        else:\n",
        "          layer = self._prop_layers[0]\n",
        "        self._prop_layers.append(layer)\n",
        "\n",
        "    node_features, edge_features = self._encoder(node_features, edge_features)\n",
        "    node_states = node_features\n",
        "\n",
        "    layer_outputs = [node_states]\n",
        "\n",
        "    for layer in self._prop_layers:\n",
        "      # node_features could be wired in here as well, leaving it out for now as\n",
        "      # it is already in the inputs\n",
        "      node_states = self._apply_layer(\n",
        "          layer,\n",
        "          node_states,\n",
        "          from_idx,\n",
        "          to_idx,\n",
        "          graph_idx,\n",
        "          n_graphs,\n",
        "          edge_features)\n",
        "      layer_outputs.append(node_states)\n",
        "\n",
        "    # these tensors may be used e.g. for visualization\n",
        "    self._layer_outputs = layer_outputs\n",
        "    return self._aggregator(node_states, graph_idx, n_graphs)\n",
        "\n",
        "  def reset_n_prop_layers(self, n_prop_layers):\n",
        "    \"\"\"Set n_prop_layers to the provided new value.\n",
        "\n",
        "    This allows us to train with certain number of propagation layers and\n",
        "    evaluate with a different number of propagation layers.\n",
        "\n",
        "    This only works if n_prop_layers is smaller than the number used for\n",
        "    training, or when share_prop_params is set to True, in which case this can\n",
        "    be arbitrarily large.\n",
        "\n",
        "    Args:\n",
        "      n_prop_layers: the new number of propagation layers to set.\n",
        "    \"\"\"\n",
        "    self._n_prop_layers = n_prop_layers\n",
        "\n",
        "  @property\n",
        "  def n_prop_layers(self):\n",
        "    return self._n_prop_layers\n",
        "\n",
        "  def get_layer_outputs(self):\n",
        "    \"\"\"Get the outputs at each layer.\"\"\"\n",
        "    if hasattr(self, '_layer_outputs'):\n",
        "      return self._layer_outputs\n",
        "    else:\n",
        "      raise ValueError('No layer outputs available.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVUVjA1TfaDN"
      },
      "source": [
        "### The graph matching networks\n",
        "\n",
        "The graph matching networks (GMNs) compute the similarity score for a pair of graphs jointly on the pair.  In our current formulation, it still computes a representation for each graph, but the representations for a pair of graphs are computed jointly on the pair, through a cross-graph attention-based matching mechanism.\n",
        "\n",
        "More concretely, the graph matching model can be formulated as\n",
        "\n",
        "$$d(G_1, G_2) = d_H(embed\\_and\\_match(G_1, G_2))$$\n",
        "\n",
        "where $embed\\_and\\_match(G_1, G_2)$ returns a pair of graph representations.\n",
        "\n",
        "Similar to the embedding model, our GMNs computes graph representations through 3 steps.  The difference to the embedding model is in the message passing step, where each node not only gets messages from within the same graph, but also gets cross-graph messages by attending to all the nodes in the other graph.  This can be formulated as follows.\n",
        "\n",
        "We first have within-graph messages as before:\n",
        "$$\n",
        "m_{i\\rightarrow j} = f_\\mathrm{message}(h_i^{(t)}, h_j^{(t)}, e_{ij}).\n",
        "$$\n",
        "\n",
        "In addition, we also allow each node in one graph to attend to all the other nodes in the other graph.  The cross graph attention weight (node $i$ in one graph attending to node $j$ in the other graph, and vice versa) is computed as\n",
        "$$\\begin{array}{rcl}\n",
        "a_{i\\rightarrow j} &=& \\frac{\\exp(s(h_i^{(t)}, h_j^{(t)}))}{\\sum_j \\exp(s(h_i^{(t)}, h_j^{(t)}))} \\\\\n",
        "a_{j\\rightarrow i} &=& \\frac{\\exp(s(h_i^{(t)}, h_j^{(t)}))}{\\sum_i \\exp(s(h_i^{(t)}, h_j^{(t)}))},\n",
        "\\end{array}\n",
        "$$\n",
        "where $s(., .)$ is again a vector space similarity function, like Euclidean, dot-product or cosine.  Also note the different indices being summed over in the normalizers.\n",
        "\n",
        "The cross-graph message is then computed as\n",
        "$$\\begin{array}{rcl}\n",
        "\\mu_i &=& \\sum_j a_{i\\rightarrow j} (h_i^{(t)} - h_j^{(t)}) = h_i^{(t)} - \\sum_j a_{i\\rightarrow j} h_j^{(t)}, \\\\\n",
        "\\mu_j &=& \\sum_i a_{j\\rightarrow i} (h_j^{(t)} - h_i^{(t)}) = h_j^{(t)} - \\sum_i a_{j\\rightarrow i} h_i^{(t)}.\n",
        "\\end{array}\n",
        "$$\n",
        "Here we are computing an attention-weighted sum of all the node representations from the other graph, and then take the difference.  This is essentially **matching** one node in one graph to nodes most similar to it in the other graph, and then compute the difference.\n",
        "\n",
        "The node updates are then computed as\n",
        "$$\n",
        "h_i^{(t+1)} = f_\\mathrm{node}\\left(h_i^{(t)}, \\sum_{j:(j,i)\\in E} m_{j\\rightarrow i}, \\mu_i\\right).\n",
        "$$\n",
        "\n",
        "The graph encoder and the graph aggregators are the same as in the embedding model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAcqRtr4mvbK"
      },
      "source": [
        "#### A few similarity functions\n",
        "\n",
        "These are the functions $s(., .)$ that will be used in the cross-graph attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vhZWiK6vzDGW"
      },
      "outputs": [],
      "source": [
        "def pairwise_euclidean_similarity(x, y):\n",
        "  \"\"\"Compute the pairwise Euclidean similarity between x and y.\n",
        "\n",
        "  This function computes the following similarity value between each pair of x_i\n",
        "  and y_j: s(x_i, y_j) = -|x_i - y_j|^2.\n",
        "\n",
        "  Args:\n",
        "    x: NxD float tensor.\n",
        "    y: MxD float tensor.\n",
        "\n",
        "  Returns:\n",
        "    s: NxM float tensor, the pairwise euclidean similarity.\n",
        "  \"\"\"\n",
        "  s = 2 * tf.matmul(x, y, transpose_b=True)\n",
        "  diag_x = tf.reduce_sum(x * x, axis=-1, keepdims=True)\n",
        "  diag_y = tf.reshape(tf.reduce_sum(y * y, axis=-1), (1, -1))\n",
        "  return s - diag_x - diag_y\n",
        "\n",
        "\n",
        "def pairwise_dot_product_similarity(x, y):\n",
        "  \"\"\"Compute the dot product similarity between x and y.\n",
        "\n",
        "  This function computes the following similarity value between each pair of x_i\n",
        "  and y_j: s(x_i, y_j) = x_i^T y_j.\n",
        "\n",
        "  Args:\n",
        "    x: NxD float tensor.\n",
        "    y: MxD float tensor.\n",
        "\n",
        "  Returns:\n",
        "    s: NxM float tensor, the pairwise dot product similarity.\n",
        "  \"\"\"\n",
        "  return tf.matmul(x, y, transpose_b=True)\n",
        "\n",
        "\n",
        "def pairwise_cosine_similarity(x, y):\n",
        "  \"\"\"Compute the cosine similarity between x and y.\n",
        "\n",
        "  This function computes the following similarity value between each pair of x_i\n",
        "  and y_j: s(x_i, y_j) = x_i^T y_j / (|x_i||y_j|).\n",
        "\n",
        "  Args:\n",
        "    x: NxD float tensor.\n",
        "    y: MxD float tensor.\n",
        "\n",
        "  Returns:\n",
        "    s: NxM float tensor, the pairwise cosine similarity.\n",
        "  \"\"\"\n",
        "  x = tf.nn.l2_normalize(x, axis=-1)\n",
        "  y = tf.nn.l2_normalize(y, axis=-1)\n",
        "  return tf.matmul(x, y, transpose_b=True)\n",
        "\n",
        "\n",
        "PAIRWISE_SIMILARITY_FUNCTION = {\n",
        "    'euclidean': pairwise_euclidean_similarity,\n",
        "    'dotproduct': pairwise_dot_product_similarity,\n",
        "    'cosine': pairwise_cosine_similarity,\n",
        "}\n",
        "\n",
        "\n",
        "def get_pairwise_similarity(name):\n",
        "  \"\"\"Get pairwise similarity metric by name.\n",
        "\n",
        "  Args:\n",
        "    name: string, name of the similarity metric, one of {dot-product, cosine,\n",
        "      euclidean}.\n",
        "\n",
        "  Returns:\n",
        "    similarity: a (x, y) -> sim function.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if name is not supported.\n",
        "  \"\"\"\n",
        "  if name not in PAIRWISE_SIMILARITY_FUNCTION:\n",
        "    raise ValueError('Similarity metric name \"%s\" not supported.' % name)\n",
        "  else:\n",
        "    return PAIRWISE_SIMILARITY_FUNCTION[name]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wpqaeKLJm_x4"
      },
      "source": [
        "#### The cross-graph attention\n",
        "\n",
        "We implement this cross-graph attention in batches of pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CfO-r0bdm_DB"
      },
      "outputs": [],
      "source": [
        "def compute_cross_attention(x, y, sim):\n",
        "  \"\"\"Compute cross attention.\n",
        "\n",
        "  x_i attend to y_j:\n",
        "  a_{i->j} = exp(sim(x_i, y_j)) / sum_j exp(sim(x_i, y_j))\n",
        "  y_j attend to x_i:\n",
        "  a_{j->i} = exp(sim(x_i, y_j)) / sum_i exp(sim(x_i, y_j))\n",
        "  attention_x = sum_j a_{i->j} y_j\n",
        "  attention_y = sum_i a_{j->i} x_i\n",
        "\n",
        "  Args:\n",
        "    x: NxD float tensor.\n",
        "    y: MxD float tensor.\n",
        "    sim: a (x, y) -> similarity function.\n",
        "\n",
        "  Returns:\n",
        "    attention_x: NxD float tensor.\n",
        "    attention_y: NxD float tensor.\n",
        "  \"\"\"\n",
        "  a = sim(x, y)\n",
        "  a_x = tf.nn.softmax(a, axis=1)  # i->j\n",
        "  a_y = tf.nn.softmax(a, axis=0)  # j->i\n",
        "  attention_x = tf.matmul(a_x, y)\n",
        "  attention_y = tf.matmul(a_y, x, transpose_a=True)\n",
        "  return attention_x, attention_y\n",
        "\n",
        "\n",
        "def batch_block_pair_attention(data,\n",
        "                               block_idx,\n",
        "                               n_blocks,\n",
        "                               similarity='dotproduct'):\n",
        "  \"\"\"Compute batched attention between pairs of blocks.\n",
        "\n",
        "  This function partitions the batch data into blocks according to block_idx.\n",
        "  For each pair of blocks, x = data[block_idx == 2i], and\n",
        "  y = data[block_idx == 2i+1], we compute\n",
        "\n",
        "  x_i attend to y_j:\n",
        "  a_{i->j} = exp(sim(x_i, y_j)) / sum_j exp(sim(x_i, y_j))\n",
        "  y_j attend to x_i:\n",
        "  a_{j->i} = exp(sim(x_i, y_j)) / sum_i exp(sim(x_i, y_j))\n",
        "\n",
        "  and\n",
        "\n",
        "  attention_x = sum_j a_{i->j} y_j\n",
        "  attention_y = sum_i a_{j->i} x_i.\n",
        "\n",
        "  Args:\n",
        "    data: NxD float tensor.\n",
        "    block_idx: N-dim int tensor.\n",
        "    n_blocks: integer.\n",
        "    similarity: a string, the similarity metric.\n",
        "\n",
        "  Returns:\n",
        "    attention_output: NxD float tensor, each x_i replaced by attention_x_i.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if n_blocks is not an integer or not a multiple of 2.\n",
        "  \"\"\"\n",
        "  if not isinstance(n_blocks, int):\n",
        "    raise ValueError('n_blocks (%s) has to be an integer.' % str(n_blocks))\n",
        "\n",
        "  if n_blocks % 2 != 0:\n",
        "    raise ValueError('n_blocks (%d) must be a multiple of 2.' % n_blocks)\n",
        "\n",
        "  sim = get_pairwise_similarity(similarity)\n",
        "\n",
        "  results = []\n",
        "\n",
        "  # This is probably better than doing boolean_mask for each i\n",
        "  partitions = tf.dynamic_partition(data, block_idx, n_blocks)\n",
        "\n",
        "  # It is rather complicated to allow n_blocks be a tf tensor and do this in a\n",
        "  # dynamic loop, and probably unnecessary to do so.  Therefore we are\n",
        "  # restricting n_blocks to be a integer constant here and using the plain for\n",
        "  # loop.\n",
        "  for i in range(0, n_blocks, 2):\n",
        "    x = partitions[i]\n",
        "    y = partitions[i + 1]\n",
        "    attention_x, attention_y = compute_cross_attention(x, y, sim)\n",
        "    results.append(attention_x)\n",
        "    results.append(attention_y)\n",
        "\n",
        "  results = tf.concat(results, axis=0)\n",
        "  # the shape of the first dimension is lost after concat, reset it back\n",
        "  results.set_shape(data.shape)\n",
        "  return results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AAj9kMygnPch"
      },
      "source": [
        "#### Graph matching layer and graph matching networks\n",
        "\n",
        "This only involves a small set of changes from the graph embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gzfRNzF6nOyZ"
      },
      "outputs": [],
      "source": [
        "class GraphPropMatchingLayer(GraphPropLayer):\n",
        "  \"\"\"A graph propagation layer that also does cross graph matching.\n",
        "\n",
        "  It assumes the incoming graph data is batched and paired, i.e. graph 0 and 1\n",
        "  forms the first pair and graph 2 and 3 are the second pair etc., and computes\n",
        "  cross-graph attention-based matching for each pair.\n",
        "  \"\"\"\n",
        "\n",
        "  def _build(self,\n",
        "             node_states,\n",
        "             from_idx,\n",
        "             to_idx,\n",
        "             graph_idx,\n",
        "             n_graphs,\n",
        "             similarity='dotproduct',\n",
        "             edge_features=None,\n",
        "             node_features=None):\n",
        "    \"\"\"Run one propagation step with cross-graph matching.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, node states.\n",
        "      from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "      to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "      graph_idx: [n_onodes] int tensor, graph id for each node.\n",
        "      n_graphs: integer, number of graphs in the batch.\n",
        "      similarity: type of similarity to use for the cross graph attention.\n",
        "      edge_features: if not None, should be [n_edges, edge_feat_dim] tensor,\n",
        "        extra edge features.\n",
        "      node_features: if not None, should be [n_nodes, node_feat_dim] tensor,\n",
        "        extra node features.\n",
        "\n",
        "    Returns:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, new node states.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if some options are not provided correctly.\n",
        "    \"\"\"\n",
        "    aggregated_messages = self._compute_aggregated_messages(\n",
        "        node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "    # new stuff here\n",
        "    cross_graph_attention = batch_block_pair_attention(\n",
        "        node_states, graph_idx, n_graphs, similarity=similarity)\n",
        "    attention_input = node_states - cross_graph_attention\n",
        "\n",
        "    return self._compute_node_update(node_states,\n",
        "                                     [aggregated_messages, attention_input],\n",
        "                                     node_features=node_features)\n",
        "\n",
        "\n",
        "class GraphMatchingNet(GraphEmbeddingNet):\n",
        "  \"\"\"Graph matching net.\n",
        "\n",
        "  This class uses graph matching layers instead of the simple graph prop layers.\n",
        "\n",
        "  It assumes the incoming graph data is batched and paired, i.e. graph 0 and 1\n",
        "  forms the first pair and graph 2 and 3 are the second pair etc., and computes\n",
        "  cross-graph attention-based matching for each pair.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               encoder,\n",
        "               aggregator,\n",
        "               node_state_dim,\n",
        "               edge_hidden_sizes,\n",
        "               node_hidden_sizes,\n",
        "               n_prop_layers,\n",
        "               share_prop_params=False,\n",
        "               edge_net_init_scale=0.1,\n",
        "               node_update_type='residual',\n",
        "               use_reverse_direction=True,\n",
        "               reverse_dir_param_different=True,\n",
        "               layer_norm=False,\n",
        "               similarity='dotproduct',\n",
        "               name='graph-matching-net'):\n",
        "    super(GraphMatchingNet, self).__init__(\n",
        "        encoder,\n",
        "        aggregator,\n",
        "        node_state_dim,\n",
        "        edge_hidden_sizes,\n",
        "        node_hidden_sizes,\n",
        "        n_prop_layers,\n",
        "        share_prop_params=share_prop_params,\n",
        "        edge_net_init_scale=edge_net_init_scale,\n",
        "        node_update_type=node_update_type,\n",
        "        use_reverse_direction=use_reverse_direction,\n",
        "        reverse_dir_param_different=reverse_dir_param_different,\n",
        "        layer_norm=layer_norm,\n",
        "        name=name)\n",
        "    self._similarity = similarity\n",
        "    self._layer_class = GraphPropMatchingLayer\n",
        "\n",
        "  def _apply_layer(self,\n",
        "                   layer,\n",
        "                   node_states,\n",
        "                   from_idx,\n",
        "                   to_idx,\n",
        "                   graph_idx,\n",
        "                   n_graphs,\n",
        "                   edge_features):\n",
        "    \"\"\"Apply one layer on the given inputs.\"\"\"\n",
        "    return layer(node_states, from_idx, to_idx, graph_idx, n_graphs,\n",
        "                 similarity=self._similarity, edge_features=edge_features)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tBXC2yUgzlyO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EVVXhPc6zqAy"
      },
      "source": [
        "### Labeled data examples\n",
        "\n",
        "We train on either pairs of graphs or triplets of graphs.  For pairs of graphs, we assume each pair $(G_1, G_2)$ comes with a label $t\\in\\{-1, 1\\}$.  $t=1$ if $G_1$ and $G_2$ are similar, and $t=-1$ otherwise.\n",
        "\n",
        "For triplets of graphs, we assume within each triplet $(G_1, G_2, G_3)$, $G_1$ is similar to $G_2$ but not similar to $G_3$.\n",
        "\n",
        "The goal of training is to learn the parameters of the function $f(G_1, G_2)$ such that similar graphs have high similarity (or small distance) and dissimilar graphs have low similarity (or high distance)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8jSKnn5G0KgL"
      },
      "source": [
        "### Training on pairs\n",
        "\n",
        "Given a dataset of pairs $(G_1, G_2)$ and labels $t\\in\\{-1, 1\\}$, we can use the following margin-based loss if using Euclidean distance:\n",
        "\n",
        "$$\n",
        "L_\\mathrm{pair} = \\mathbb{E}_{(G_1, G_2, t)}[\\max\\{0, \\gamma - t(1 - d(G_1, G_2))\\}]\n",
        "$$\n",
        "\n",
        "This loss encourages similar graphs to have distance smaller than $1-\\gamma$, and dissimilar graphs to have distance greater than $1 + \\gamma$, where $\\gamma$ is a margin parameter.\n",
        "\n",
        "Alternatively, for many applications it is beneficial to have the representation of graphs be binary which allows efficient indexing and hashing.  In this case, Hamming distance (similarity) is more appropriate.  On the other hand, the Hamming distance is not differentiable, so we use a smooth approximation\n",
        "$$\n",
        "s(G_1, G_2) = \\frac{1}{H}\\sum_{i=1}^H \\tanh(h_{G_1, i}) \\cdot \\tanh(h_{G_2, i}),\n",
        "$$\n",
        "where $s$ is now a similarity (rather than distance) function, $h_{G, i}$ is the i-th dimension of the smooth representation vector for G.  We get binary codes by thresholding $h_{G,i}$ at 0, i.e. $\\hat{h}_{G,i}=1$ if $h_{G,i}\\ge 0$ and $-1$ otherwise.\n",
        "\n",
        "The loss we use with these binary representations is defined as\n",
        "\n",
        "$$\n",
        "L_\\mathrm{pair} = \\mathbb{E}_{(G_1, G_2, t)}[(t - s(G_1, G_2))^2] / 4.\n",
        "$$\n",
        "\n",
        "The factor of $1/4$ is used to normalize the loss to between 0 and 1.\n",
        "\n",
        "These are just two possible losses, many other types of losses could also be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vwqL12jS2t2V"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(x, y):\n",
        "  \"\"\"This is the squared Euclidean distance.\"\"\"\n",
        "  return tf.reduce_sum((x - y)**2, axis=-1)\n",
        "\n",
        "\n",
        "def approximate_hamming_similarity(x, y):\n",
        "  \"\"\"Approximate Hamming similarity.\"\"\"\n",
        "  return tf.reduce_mean(tf.tanh(x) * tf.tanh(y), axis=1)\n",
        "\n",
        "\n",
        "def pairwise_loss(x, y, labels, loss_type='margin', margin=1.0):\n",
        "  \"\"\"Compute pairwise loss.\n",
        "\n",
        "  Args:\n",
        "    x: [N, D] float tensor, representations for N examples.\n",
        "    y: [N, D] float tensor, representations for another N examples.\n",
        "    labels: [N] int tensor, with values in -1 or +1.  labels[i] = +1 if x[i]\n",
        "      and y[i] are similar, and -1 otherwise.\n",
        "    loss_type: margin or hamming.\n",
        "    margin: float scalar, margin for the margin loss.\n",
        "\n",
        "  Returns:\n",
        "    loss: [N] float tensor.  Loss for each pair of representations.\n",
        "  \"\"\"\n",
        "  labels = tf.cast(labels, x.dtype)\n",
        "  if loss_type == 'margin':\n",
        "    return tf.nn.relu(margin - labels * (1 - euclidean_distance(x, y)))\n",
        "  elif loss_type == 'hamming':\n",
        "    return 0.25 * (labels - approximate_hamming_similarity(x, y))**2\n",
        "  else:\n",
        "    raise ValueError('Unknown loss_type %s' % loss_type)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1-k-A4e2uV8"
      },
      "source": [
        "### Training on triplets\n",
        "\n",
        "Given a dataset of triplets $(G_1, G_2, G_3)$ where $G_1$ is similar to $G_2$ but not similar to $G_3$, we can again use a margin loss or Hamming similarity-based loss.\n",
        "\n",
        "The following margin loss can be used with Euclidean distance:\n",
        "\n",
        "$$\n",
        "L_\\mathrm{triplet} = \\mathbb{E}_{(G_1, G_2, G_3)}[\\max\\{0, d(G_1, G_2) - d(G_1, G_3) + \\gamma\\}],\n",
        "$$\n",
        "which encourages $d(G_1, G_2)$ to be smaller by $d(G_1, G_3)$ by at least a margin of $\\gamma$.\n",
        "\n",
        "If using Hamming distance (similarity) we could use the following loss:\n",
        "\n",
        "$$\n",
        "L_\\mathrm{triplet} = \\mathbb{E}_{(G_1, G_2, G_3)}[(s(G_1, G_2) - 1)^2 + (s(G_1, G_3) + 1)^2] / 8\n",
        "$$\n",
        "\n",
        "The factor of $1/8$ is again used to normalize the loss to within 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7ibuK0-y4WcI"
      },
      "outputs": [],
      "source": [
        "def triplet_loss(x_1, y, x_2, z, loss_type='margin', margin=1.0):\n",
        "  \"\"\"Compute triplet loss.\n",
        "\n",
        "  This function computes loss on a triplet of inputs (x, y, z).  A similarity or\n",
        "  distance value is computed for each pair of (x, y) and (x, z).  Since the\n",
        "  representations for x can be different in the two pairs (like our matching\n",
        "  model) we distinguish the two x representations by x_1 and x_2.\n",
        "\n",
        "  Args:\n",
        "    x_1: [N, D] float tensor.\n",
        "    y: [N, D] float tensor.\n",
        "    x_2: [N, D] float tensor.\n",
        "    z: [N, D] float tensor.\n",
        "    loss_type: margin or hamming.\n",
        "    margin: float scalar, margin for the margin loss.\n",
        "\n",
        "  Returns:\n",
        "    loss: [N] float tensor.  Loss for each pair of representations.\n",
        "  \"\"\"\n",
        "  if loss_type == 'margin':\n",
        "    return tf.nn.relu(margin +\n",
        "                      euclidean_distance(x_1, y) -\n",
        "                      euclidean_distance(x_2, z))\n",
        "  elif loss_type == 'hamming':\n",
        "    return 0.125 * ((approximate_hamming_similarity(x_1, y) - 1)**2 +\n",
        "                    (approximate_hamming_similarity(x_2, z) + 1)**2)\n",
        "  else:\n",
        "    raise ValueError('Unknown loss_type %s' % loss_type)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQkcwBrdn4YZ"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "We use an abstract `GraphSimilarityDataset` class to define the general interface for the training and evaluatin data used in graph similarity learning.\n",
        "\n",
        "We also assume a certain format for packing the graphs into tensors as described below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6lAkeOZnn32w"
      },
      "outputs": [],
      "source": [
        "GraphData = collections.namedtuple('GraphData', [\n",
        "    'from_idx',\n",
        "    'to_idx',\n",
        "    'node_features',\n",
        "    'edge_features',\n",
        "    'graph_idx',\n",
        "    'n_graphs'])\n",
        "\n",
        "\n",
        "@six.add_metaclass(abc.ABCMeta)\n",
        "class GraphSimilarityDataset(object):\n",
        "  \"\"\"Base class for all the graph similarity learning datasets.\n",
        "\n",
        "  This class defines some common interfaces a graph similarity dataset can have,\n",
        "  in particular the functions that creates iterators over pairs and triplets.\n",
        "  \"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def triplets(self, batch_size):\n",
        "    \"\"\"Create an iterator over triplets.\n",
        "\n",
        "    Args:\n",
        "      batch_size: int, number of triplets in a batch.\n",
        "\n",
        "    Yields:\n",
        "      graphs: a `GraphData` instance.  The batch of triplets put together.  Each\n",
        "        triplet has 3 graphs (x, y, z).  Here the first graph is duplicated once\n",
        "        so the graphs for each triplet are ordered as (x, y, x, z) in the batch.\n",
        "        The batch contains `batch_size` number of triplets, hence `4*batch_size`\n",
        "        many graphs.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def pairs(self, batch_size):\n",
        "    \"\"\"Create an iterator over pairs.\n",
        "\n",
        "    Args:\n",
        "      batch_size: int, number of pairs in a batch.\n",
        "\n",
        "    Yields:\n",
        "      graphs: a `GraphData` instance.  The batch of pairs put together.  Each\n",
        "        pair has 2 graphs (x, y).  The batch contains `batch_size` number of\n",
        "        pairs, hence `2*batch_size` many graphs.\n",
        "      labels: [batch_size] int labels for each pair, +1 for similar, -1 for not.\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PPKfGwXPzFMW"
      },
      "source": [
        "## The graph edit distance task"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "328H0viCAwyG"
      },
      "source": [
        "Here we use the synthetic task of learning approximate graph edit distances (GEDs) to test the effectiveness of the graph similarity learning models.\n",
        "\n",
        "Given two graphs $G_1$ and $G_2$, we say the edit distance between them is the number of actions needed to transform one into another.  Since the difference in graph size (number of nodes, number of edges) is trivial to identify, we only consider the case of substituting one edge with another edge, and define the edit distance between two equal-size graphs to be the number of edge changes needed to make them identical (or isomorphic).\n",
        "\n",
        "We train a graph similarity or distance model that aligns with the GED, by giving graphs with small edit distance a high similarity score or low learned distance, and otherwise low similarity score or high learned distance.  Note that we don't try to match the learned graph distance with the actual GED, as we don't assume the exact scale of the GED is available during training.\n",
        "\n",
        "To get training data, we sample graph $G$ from the Erdos-Renyi model (other random graph models could be used as well), and then:\n",
        "* apply a node permutation to $G$ to get $G_1$,\n",
        "* apply $k_1$ edge changes to $G$ to get $G_2$,\n",
        "* apply $k_2$ edge changes to $G$ to get $G_3$.\n",
        "\n",
        "Each edge change action substitutes one edge $(i,j)$ by another edge $(i', j')$ where $i'$ and $j'$ are picked randomly.  In this case, we have roughly $GED(G_1, G_2)\\approx k_1$ and $GED(G_1, G_3)\\approx k_2$.  We make $k_1 < k_2$ and say $(G_1, G_2)$ is a positive pair that are similar and $(G_1, G_3)$ is a negative pair hence not similar.\n",
        "\n",
        "Note that the above GED for $G_1, G_2$ and $G_3$ only holds approximately, and $GED(G_1, G_2)\\le k_1$ and $GED(G_1, G_3)\\le k_2$, because of the potential symmetry in $G$, i.e. it is possible that changing one edge may only change the graph into another isomorphic graph.  However the probability of this happening is relatively small and decreases with increasing graph size.  So we ignore this possibility in this task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U_68U4wIo7lT"
      },
      "source": [
        "### A few graph manipulation primitives\n",
        "\n",
        "These primitives assume the incoming graphs are instances of `networkx.Graph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZyJh0DGNyWox"
      },
      "outputs": [],
      "source": [
        "def permute_graph_nodes(g):\n",
        "  \"\"\"Permute node ordering of a graph, returns a new graph.\"\"\"\n",
        "  n = g.number_of_nodes()\n",
        "  new_g = nx.Graph()\n",
        "  new_g.add_nodes_from(range(n))\n",
        "  perm = np.random.permutation(n)\n",
        "  edges = g.edges()\n",
        "  new_edges = []\n",
        "  for x, y in edges:\n",
        "    new_edges.append((perm[x], perm[y]))\n",
        "  new_g.add_edges_from(new_edges)\n",
        "  return new_g\n",
        "\n",
        "\n",
        "def substitute_random_edges(g, n):\n",
        "  \"\"\"Substitutes n edges from graph g with another n randomly picked edges.\"\"\"\n",
        "  g = copy.deepcopy(g)\n",
        "  n_nodes = g.number_of_nodes()\n",
        "  edges = list(g.edges())\n",
        "  # sample n edges without replacement\n",
        "  e_remove = [edges[i] for i in np.random.choice(\n",
        "      np.arange(len(edges)), n, replace=False)]\n",
        "  edge_set = set(edges)\n",
        "  e_add = set()\n",
        "  while len(e_add) < n:\n",
        "    e = np.random.choice(n_nodes, 2, replace=False)\n",
        "    # make sure e does not exist and is not already chosen to be added\n",
        "    if ((e[0], e[1]) not in edge_set and (e[1], e[0]) not in edge_set and\n",
        "        (e[0], e[1]) not in e_add and (e[1], e[0]) not in e_add):\n",
        "      e_add.add((e[0], e[1]))\n",
        "\n",
        "  for i, j in e_remove:\n",
        "    g.remove_edge(i, j)\n",
        "  for i, j in e_add:\n",
        "    g.add_edge(i, j)\n",
        "  return g"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ECXrG7VJpIDl"
      },
      "source": [
        "### Dataset for IE Models\n",
        "*#DSB Modification*\n",
        "\n",
        "For both training and validation dataset we use the AttributedGraphDataset class. The class supports GMN training via the pairs method which is explained by deepmind above. The *_pack_batch* method included within the class is a modification of the *_pack_batch* method from the deepmind GraphEditDistanceDataset example so that the data returned from the AtributedGraphDataset is identical in format.\n",
        "\n",
        "To load an IE model dataset simple call:\n",
        "```\n",
        "    training_set, validation_set, test_set = AttributedGraphDataset.load_dataset(**parameters**)\n",
        "```\n",
        "\n",
        "This will load the dataset, randomly split the data into a training, validation and test set and return the aformentioned sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttributedGraphDataset(GraphSimilarityDataset):\n",
        "    \"\"\"Attributed Graph Dataset\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor\n",
        "        \n",
        "        Initialises populations, graphs, labels and pairs storage lists\n",
        "        \"\"\"\n",
        "        # Graph Data Storage\n",
        "        self._ids = []\n",
        "        self._populations = []\n",
        "        self._graphs = []\n",
        "        self._node_features = []\n",
        "        self._edge_features = []\n",
        "        # Pair Data Storage\n",
        "        self._names = []\n",
        "        self._labels = []\n",
        "        self._pairs = []\n",
        "        self._paired_node_features = []\n",
        "        self._paired_edge_features = []\n",
        "\n",
        "    def add_graph(self, id: str, population: str, graph: nx.graph, node_features: list, edge_features: list):\n",
        "        \"\"\"Add Graph to Dataset\n",
        "\n",
        "        'id': graph name\n",
        "        'population': string of which population the corresponding graph belongs to\n",
        "        'graph': a networkx graph representation of the IE model\n",
        "        \"\"\"\n",
        "    \n",
        "        # Generate Pairs & Append\n",
        "        for i in range(0, len(self._graphs)):\n",
        "            # Add G1 -> G2 pair\n",
        "            self._names.append((self._ids[i], id))\n",
        "            self._labels.append(1 if self._populations[i] == population else -1)\n",
        "            self._pairs.append((self._graphs[i], graph))\n",
        "            self._paired_node_features.append(np.concatenate((self._node_features[i], node_features)))\n",
        "            self._paired_edge_features.append(np.concatenate((self._edge_features[i], edge_features)))\n",
        "            # Add G2 -> G1 pair\n",
        "            self._names.append((id, self._ids[i]))\n",
        "            self._labels.append(1 if population == self._populations[i] else -1)\n",
        "            self._pairs.append((graph, self._graphs[i]))\n",
        "            self._paired_node_features.append(np.concatenate((node_features, self._node_features[i])))\n",
        "            self._paired_edge_features.append(np.concatenate((edge_features, self._edge_features[i])))\n",
        "            \n",
        "        # Add G1 -> G1\n",
        "        self._names.append((id, id))\n",
        "        self._labels.append(1)\n",
        "        self._pairs.append((graph, graph))\n",
        "        self._paired_node_features.append(np.concatenate((node_features, node_features)))\n",
        "        self._paired_edge_features.append(np.concatenate((edge_features, edge_features)))\n",
        "            \n",
        "        # Add Storge\n",
        "        self._ids.append(id)\n",
        "        self._populations.append(population)\n",
        "        self._graphs.append(graph)\n",
        "        self._node_features.append(node_features)\n",
        "        self._edge_features.append(edge_features)\n",
        "    \n",
        "    def triplets(self, batch_size: int):\n",
        "        \"\"\"Triplet data training\n",
        "        \n",
        "        'batch_size': number of triplets to return\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Attributed Graph Dataset does not currently support triplet matching\")\n",
        "\n",
        "    def pairs(self, batch_size: int):\n",
        "        \"\"\"Pair data training\n",
        "        \n",
        "        'batch_size': number of pairs to return\n",
        "        \"\"\"\n",
        "        ptr = 0\n",
        "        labels = np.array(self._labels, dtype=np.int32)\n",
        "        while ptr + batch_size <= len(self._pairs):\n",
        "            batch_graphs = self._pairs[ptr:ptr + batch_size]\n",
        "            batch_node_features = self._paired_node_features[ptr:ptr + batch_size]\n",
        "            batch_edge_features = self._paired_edge_features[ptr:ptr + batch_size]\n",
        "            packed_batch_graphs = AttributedGraphDataset._pack_batch(batch_graphs, batch_node_features, batch_edge_features)\n",
        "            yield packed_batch_graphs, labels[ptr:ptr + batch_size]\n",
        "            ptr += batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_attributed_graph(elements: list, relationships: list):\n",
        "        \"\"\"Generate networkx graph representation of an IE model\n",
        "        \n",
        "        'elements': a list of IE model elements\n",
        "        'relationships': a list of IE model relationships\n",
        "        return: (networkx graph, node_feature, edge_features)\n",
        "        \"\"\"\n",
        "        # Add Nodes\n",
        "        graph = nx.Graph()\n",
        "        for node in elements:\n",
        "            graph.add_node(node[\"name\"])\n",
        "        # Add Edges\n",
        "        for edge in relationships:\n",
        "            graph.add_edge(\n",
        "                edge[\"elements\"][0][\"name\"],\n",
        "                edge[\"elements\"][1][\"name\"]\n",
        "            )\n",
        "        # Generate Node Features\n",
        "        type_map ={\"regular\":0, \"ground\":1}\n",
        "        contextual_map = { \"fuselage\":0, \"wing\":1, \"aerofoil\":2}\n",
        "        material_map = {\n",
        "            (\"metal\", \"ferrousAlloy\"): 0,\n",
        "            (\"metal\", \"aluminiumAlloy\"): 1,\n",
        "            (\"metal\", \"nickelAlloy\"): 2,\n",
        "            (\"metal\", \"copperAlloy\"): 3,\n",
        "            (\"metal\", \"titaniumAlloy\"): 4,\n",
        "            (\"composite\", \"fibre-reinforced\"): 5,\n",
        "            (\"ceramic\", \"cement\"): 6}\n",
        "        geometry_map = {   \n",
        "            (\"beam\",\"rectangular\"):0,\n",
        "            (\"beam\", \"circular\"):1,\n",
        "            (\"plate\",\"rectangular\"):2,\n",
        "            (\"shell\",\"translate\",\"cuboid\"):3,\n",
        "            (\"shell\",\"translate\",\"cylinder\"):4,\n",
        "            (\"solid\",\"translate\",\"cuboid\"):5,\n",
        "            (\"solid\",\"translate\",\"cylinder\"):6}\n",
        "\n",
        "        number_of_node_features = len(type_map) + len(contextual_map) + len(material_map) + len(geometry_map) +1\n",
        "        nodes = graph.nodes()\n",
        "        node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "        node_features = np.zeros((len(nodes), number_of_node_features))\n",
        "        \n",
        "        \n",
        "        for node_name in node_mapping:\n",
        "            for element in elements:\n",
        "                if node_name == element[\"name\"]:\n",
        "                    # First Feature: Element Type\n",
        "                    node_features[node_mapping[node_name]][type_map[element[\"type\"]]] = 1\n",
        "                    #node_features[node_mapping[node_name]][0] = 2 if element[\"type\"] == \"regular\" else 1\n",
        "                    \n",
        "                    # Second Feature: Element Contextual\n",
        "                    node_features[node_mapping[node_name]][contextual_map[element[\"contextual\"]['type']]+len(type_map)] = 1\n",
        "                    #node_features[node_mapping[node_name]][1] = contextual_map.get(element[\"contextual\"]['type'],len(contextual_map)+1)\n",
        "                    \n",
        "                    # Third Feature: Element Material\n",
        "                    node_features[node_mapping[node_name]][material_map[(element[\"material\"][\"type\"][\"name\"],element[\"material\"][\"type\"][\"type\"][\"name\"])]+len(type_map)+len(contextual_map)] = 1\n",
        "                                                        \n",
        "                    # Fourth Feature: Element Geometry\n",
        "                    if len(element[\"geometry\"][\"type\"][\"type\"])<2:             \n",
        "                        node_features[node_mapping[node_name]][geometry_map[(element[\"geometry\"][\"type\"][\"name\"],element[\"geometry\"][\"type\"][\"type\"][\"name\"])]+len(type_map)+len(contextual_map)+len(material_map)] = 1\n",
        "                    else:\n",
        "                        node_features[node_mapping[node_name]][geometry_map[(element[\"geometry\"][\"type\"][\"name\"],element[\"geometry\"][\"type\"][\"type\"][\"name\"],element[\"geometry\"][\"type\"][\"type\"][\"type\"][\"name\"])]+len(type_map)+len(contextual_map)+len(material_map)] = 1\n",
        "                    \n",
        "                    # Fifth Feature: ELement Dimension\n",
        "                    max_length = 2750\n",
        "                    node_features[node_mapping[node_name]][-1] = element[\"geometry\"][\"dimensions\"]['length']['value']/max_length\n",
        "                    \n",
        "        # Generate Edge Features\n",
        "        relationships_map = {\"boundary\":0,\"perfect\":1, \"connection\":2, \"joint\":3 }\n",
        "        number_of_edge_features = len(relationships_map)\n",
        "        edges = graph.edges()\n",
        "        edge_features = np.zeros((len(edges), number_of_edge_features), dtype=np.float32)\n",
        "        for i, edge in enumerate(edges):\n",
        "            for relationship in relationships:\n",
        "                if edge[0] == relationship[\"elements\"][0][\"name\"] and edge[1] == relationship[\"elements\"][1][\"name\"]:\n",
        "                    # First Feature: Relationship Type\n",
        "                    edge_features[i][relationships_map[relationship[\"type\"]]] = 1\n",
        "        return (nx.relabel_nodes(graph, node_mapping), node_features, edge_features)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _pack_batch(graphs: list, node_features: list, edge_features: list):\n",
        "        \"\"\"Pack Batch\n",
        "\n",
        "        Taken from the original deepmind code\n",
        "\n",
        "        'graphs': a list of networkx graphs\n",
        "        return: a Graph Data tuple\n",
        "        \"\"\"\n",
        "        graphs = tf.nest.flatten(graphs)\n",
        "        from_idx = []\n",
        "        to_idx = []\n",
        "        graph_idx = []\n",
        "\n",
        "        n_total_nodes = 0\n",
        "        n_total_edges = 0\n",
        "        for i, g in enumerate(graphs):\n",
        "            n_nodes = g.number_of_nodes()\n",
        "            n_edges = g.number_of_edges()\n",
        "            edges = np.array(g.edges(), dtype=np.int32)\n",
        "            # shift the node indices for the edges\n",
        "            from_idx.append(edges[:, 0] + n_total_nodes)\n",
        "            to_idx.append(edges[:, 1] + n_total_nodes)\n",
        "            graph_idx.append(np.ones(n_nodes, dtype=np.int32) * i)\n",
        "\n",
        "            n_total_nodes += n_nodes\n",
        "            n_total_edges += n_edges\n",
        "\n",
        "        return GraphData(\n",
        "            from_idx=np.concatenate(from_idx, axis=0),\n",
        "            to_idx=np.concatenate(to_idx, axis=0),\n",
        "            node_features=np.concatenate(node_features),\n",
        "            edge_features=np.concatenate(edge_features),\n",
        "            graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "            n_graphs=len(graphs)\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_dataset(folder: str, dataset: str, population_regex: str, validation_percentage: float, test_size: int, test_batch_size: int):\n",
        "        \"\"\"Load Dataset\n",
        "\n",
        "        Generates Training and Validation AttributedGraphDataset's from dataset\n",
        "        \n",
        "        'folder': a string of the base directory to find the dataset within\n",
        "        'dataset': the name of the dataset to load\n",
        "        'population_regex': the regex needed to identify the population from the IE model name\n",
        "        'validation_percentage': validation/training percentage\n",
        "        'test_size': test size\n",
        "        'test_batch_size': validation batch size\n",
        "        return: (Training Data Set, Validation Data Set)\n",
        "        \"\"\"\n",
        "        print(f\"Loading Dataset: {dataset}\")\n",
        "        # Load Dataset\n",
        "        \n",
        "        if not os.path.isdir(os.path.join(folder, dataset)):\n",
        "            raise ValueError(f\"{os.path.join(folder, dataset)} does not exist\")\n",
        "            sys.exit()\n",
        "        data = []\n",
        "        for item in os.listdir(os.path.join(folder, dataset)):\n",
        "            if not os.path.isfile(os.path.join(folder, dataset, item)):\n",
        "                continue\n",
        "            elif item.rfind(\".json\") == (len(item) - 5):\n",
        "                with open(os.path.join(folder, dataset, item)) as fs:\n",
        "                    payload = json.loads(fs.read())\n",
        "                    population = re.search(rf\"{population_regex}\", payload[\"name\"])\n",
        "                    attributed_graph = AttributedGraphDataset._generate_attributed_graph(payload[\"models\"][\"irreducibleElement\"][\"elements\"], payload[\"models\"][\"irreducibleElement\"][\"relationships\"])\n",
        "                    data.append({\"id\": payload[\"name\"], \"population\": population.group(), \"graph\": attributed_graph[0], \"node_features\": attributed_graph[1], \"edge_features\": attributed_graph[2]})\n",
        "        # Calculate Split for Training, Validation & Test\n",
        "        np.random.shuffle(data)\n",
        "        dataset_size = len(data)\n",
        "        test_split = test_size\n",
        "        while test_split ** 2 % test_batch_size != 0:\n",
        "            test_split += 1\n",
        "        validation_split = dataset_size - int(dataset_size * ((100.0 - validation_percentage) / 100.0))\n",
        "        training_split = dataset_size - validation_split - test_split\n",
        "        # Create datasets\n",
        "        training_set = AttributedGraphDataset()\n",
        "        training_set_metrics = {}\n",
        "        for i in range(0, training_split):\n",
        "            training_set.add_graph(data[i][\"id\"], data[i][\"population\"], data[i][\"graph\"],  data[i][\"node_features\"], data[i][\"edge_features\"])\n",
        "            training_set_metrics[data[i][\"population\"]] = 1 if data[i][\"population\"] not in training_set_metrics else training_set_metrics[data[i][\"population\"]] + 1\n",
        "        validation_set = AttributedGraphDataset()\n",
        "        validation_set_metrics = {}\n",
        "        for i in range(training_split, training_split + validation_split):\n",
        "            validation_set.add_graph(data[i][\"id\"], data[i][\"population\"], data[i][\"graph\"], data[i][\"node_features\"], data[i][\"edge_features\"])\n",
        "            validation_set_metrics[data[i][\"population\"]] = 1 if data[i][\"population\"] not in validation_set_metrics else validation_set_metrics[data[i][\"population\"]] + 1\n",
        "        test_set = AttributedGraphDataset()\n",
        "        test_set_metrics = {}\n",
        "        for i in range(training_split + validation_split, training_split + validation_split + test_split):\n",
        "            test_set.add_graph(data[i][\"id\"], data[i][\"population\"], data[i][\"graph\"], data[i][\"node_features\"], data[i][\"edge_features\"])\n",
        "            test_set_metrics[data[i][\"population\"]] = 1 if data[i][\"population\"] not in test_set_metrics else test_set_metrics[data[i][\"population\"]] + 1\n",
        "        # Output Diagnostics\n",
        "        print(f\"Training Set Size: {len(training_set._pairs)} ({len(training_set._ids)} individual graphs)\")\n",
        "        print(*[f\"\\tPopulation: {metric} Count: {training_set_metrics[metric]}\" for metric in training_set_metrics], sep=\"\\r\\n\")\n",
        "        print(f\"Validation Set Size: {len(validation_set._pairs)} ({len(validation_set._ids)} individual graphs)\")\n",
        "        print(*[f\"\\tPopulation: {metric} Count: {validation_set_metrics[metric]}\" for metric in validation_set_metrics], sep=\"\\r\\n\")\n",
        "        print(f\"Test Set Size: {len(test_set._pairs)} ({len(test_set._ids)} individual graphs)\")\n",
        "        print(*[f\"\\tPopulation: {metric} Count: {test_set_metrics[metric]}\" for metric in test_set_metrics], sep=\"\\r\\n\")\n",
        "\n",
        "        return training_set, validation_set, test_set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset for training, fixed dataset for evaluation (original deepmind example)\n",
        "\n",
        "For training we use a procedure to generate graphs in pairs or triplets on the fly, and wrap this process into a `Dataset` instance.  For evaluation we need to\n",
        "use a fixed set to make sure the evaluation results are consistent and comparable, and we do that by controlling random seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZW5KEb-gpHlq"
      },
      "outputs": [],
      "source": [
        "class GraphEditDistanceDataset(GraphSimilarityDataset):\n",
        "  \"\"\"Graph edit distance dataset.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               n_nodes_range,\n",
        "               p_edge_range,\n",
        "               n_changes_positive,\n",
        "               n_changes_negative,\n",
        "               permute=True):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      n_nodes_range: a tuple (n_min, n_max).  The minimum and maximum number of\n",
        "        nodes in a graph to generate.\n",
        "      p_edge_range: a tuple (p_min, p_max).  The minimum and maximum edge\n",
        "        probability.\n",
        "      n_changes_positive: the number of edge substitutions for a pair to be\n",
        "        considered positive (similar).\n",
        "      n_changes_negative: the number of edge substitutions for a pair to be\n",
        "        considered negative (not similar).\n",
        "      permute: if True (default), permute node orderings in addition to\n",
        "        changing edges; if False, the node orderings across a pair or triplet of\n",
        "        graphs will be the same, useful for visualization.\n",
        "    \"\"\"\n",
        "    print(f\"Graph Edit Distance Dataset: Nodes {n_nodes_range}, Edges {p_edge_range}, Positive {n_changes_positive}, Negative {n_changes_negative}\")\n",
        "    self._n_min, self._n_max = n_nodes_range\n",
        "    self._p_min, self._p_max = p_edge_range\n",
        "    self._k_pos = n_changes_positive\n",
        "    self._k_neg = n_changes_negative\n",
        "    self._permute = permute\n",
        "\n",
        "  def _get_graph(self):\n",
        "    \"\"\"Generate one graph.\"\"\"\n",
        "    n_nodes = np.random.randint(self._n_min, self._n_max + 1)\n",
        "    p_edge = np.random.uniform(self._p_min, self._p_max)\n",
        "\n",
        "    # do a little bit of filtering\n",
        "    n_trials = 100\n",
        "    for _ in range(n_trials):\n",
        "      g = nx.erdos_renyi_graph(n_nodes, p_edge)\n",
        "      if nx.is_connected(g):\n",
        "        return g\n",
        "\n",
        "    raise ValueError('Failed to generate a connected graph.')\n",
        "\n",
        "  def _get_pair(self, positive):\n",
        "    \"\"\"Generate one pair of graphs.\"\"\"\n",
        "    g = self._get_graph()\n",
        "    if self._permute:\n",
        "      permuted_g = permute_graph_nodes(g)\n",
        "    else:\n",
        "      permuted_g = g\n",
        "    n_changes = self._k_pos if positive else self._k_neg\n",
        "    changed_g = substitute_random_edges(g, n_changes)\n",
        "    return permuted_g, changed_g\n",
        "\n",
        "  def _get_triplet(self):\n",
        "    \"\"\"Generate one triplet of graphs.\"\"\"\n",
        "    g = self._get_graph()\n",
        "    if self._permute:\n",
        "      permuted_g = permute_graph_nodes(g)\n",
        "    else:\n",
        "      permuted_g = g\n",
        "    pos_g = substitute_random_edges(g, self._k_pos)\n",
        "    neg_g = substitute_random_edges(g, self._k_neg)\n",
        "    return permuted_g, pos_g, neg_g\n",
        "\n",
        "  def triplets(self, batch_size):\n",
        "    \"\"\"Yields batches of triplet data.\"\"\"\n",
        "    while True:\n",
        "      batch_graphs = []\n",
        "      for _ in range(batch_size):\n",
        "        g1, g2, g3 = self._get_triplet()\n",
        "        batch_graphs.append((g1, g2, g1, g3))\n",
        "      yield self._pack_batch(batch_graphs)\n",
        "\n",
        "  def pairs(self, batch_size):\n",
        "    \"\"\"Yields batches of pair data.\"\"\"\n",
        "    while True:\n",
        "      batch_graphs = []\n",
        "      batch_labels = []\n",
        "      positive = True\n",
        "      for _ in range(batch_size):\n",
        "        g1, g2 = self._get_pair(positive)\n",
        "        batch_graphs.append((g1, g2))\n",
        "        batch_labels.append(1 if positive else -1)\n",
        "        positive = not positive\n",
        "\n",
        "      packed_graphs = self._pack_batch(batch_graphs)\n",
        "      labels = np.array(batch_labels, dtype=np.int32)\n",
        "      yield packed_graphs, labels\n",
        "\n",
        "  def _pack_batch(self, graphs):\n",
        "    \"\"\"Pack a batch of graphs into a single `GraphData` instance.\n",
        "\n",
        "    Args:\n",
        "      graphs: a list of generated networkx graphs.\n",
        "\n",
        "    Returns:\n",
        "      graph_data: a `GraphData` instance, with node and edge indices properly\n",
        "        shifted.\n",
        "    \"\"\"\n",
        "    graphs = tf.nest.flatten(graphs)\n",
        "    from_idx = []\n",
        "    to_idx = []\n",
        "    graph_idx = []\n",
        "\n",
        "    n_total_nodes = 0\n",
        "    n_total_edges = 0\n",
        "    for i, g in enumerate(graphs):\n",
        "      n_nodes = g.number_of_nodes()\n",
        "      n_edges = g.number_of_edges()\n",
        "      edges = np.array(g.edges(), dtype=np.int32)\n",
        "      # shift the node indices for the edges\n",
        "      from_idx.append(edges[:, 0] + n_total_nodes)\n",
        "      to_idx.append(edges[:, 1] + n_total_nodes)\n",
        "      graph_idx.append(np.ones(n_nodes, dtype=np.int32) * i)\n",
        "\n",
        "      n_total_nodes += n_nodes\n",
        "      n_total_edges += n_edges\n",
        "\n",
        "    return GraphData(\n",
        "        from_idx=np.concatenate(from_idx, axis=0),\n",
        "        to_idx=np.concatenate(to_idx, axis=0),\n",
        "        # this task only cares about the structures, the graphs have no features\n",
        "        node_features=np.ones((n_total_nodes, 1), dtype=np.float32),\n",
        "        edge_features=np.ones((n_total_edges, 1), dtype=np.float32),\n",
        "        graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "        n_graphs=len(graphs))\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def reset_random_state(seed):\n",
        "  \"\"\"This function creates a context that uses the given seed.\"\"\"\n",
        "  np_rnd_state = np.random.get_state()\n",
        "  rnd_state = random.getstate()\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed + 1)\n",
        "  try:\n",
        "    yield\n",
        "  finally:\n",
        "    random.setstate(rnd_state)\n",
        "    np.random.set_state(np_rnd_state)\n",
        "\n",
        "\n",
        "class FixedGraphEditDistanceDataset(GraphEditDistanceDataset):\n",
        "  \"\"\"A fixed dataset of pairs or triplets for the graph edit distance task.\n",
        "\n",
        "  This dataset can be used for evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               n_nodes_range,\n",
        "               p_edge_range,\n",
        "               n_changes_positive,\n",
        "               n_changes_negative,\n",
        "               dataset_size,\n",
        "               permute=True,\n",
        "               seed=1234):\n",
        "    super(FixedGraphEditDistanceDataset, self).__init__(\n",
        "        n_nodes_range, p_edge_range, n_changes_positive, n_changes_negative,\n",
        "        permute=permute)\n",
        "    self._dataset_size = dataset_size\n",
        "    self._seed = seed\n",
        "\n",
        "  def triplets(self, batch_size):\n",
        "    \"\"\"Yield triplets.\"\"\"\n",
        "\n",
        "    if hasattr(self, '_triplets'):\n",
        "      triplets = self._triplets\n",
        "    else:\n",
        "      # get a fixed set of triplets\n",
        "      with reset_random_state(self._seed):\n",
        "        triplets = []\n",
        "        for _ in range(self._dataset_size):\n",
        "          g1, g2, g3 = self._get_triplet()\n",
        "          triplets.append((g1, g2, g1, g3))\n",
        "      self._triplets = triplets\n",
        "\n",
        "    ptr = 0\n",
        "    while ptr + batch_size <= len(triplets):\n",
        "      batch_graphs = triplets[ptr:ptr + batch_size]\n",
        "      yield self._pack_batch(batch_graphs)\n",
        "      ptr += batch_size\n",
        "\n",
        "  def pairs(self, batch_size):\n",
        "    \"\"\"Yield pairs and labels.\"\"\"\n",
        "\n",
        "    if hasattr(self, '_pairs') and hasattr(self, '_labels'):\n",
        "      pairs = self._pairs\n",
        "      labels = self._labels\n",
        "    else:\n",
        "      # get a fixed set of pairs first\n",
        "      with reset_random_state(self._seed):\n",
        "        pairs = []\n",
        "        labels = []\n",
        "        positive = True\n",
        "        for _ in range(self._dataset_size):\n",
        "          pairs.append(self._get_pair(positive))\n",
        "          labels.append(1 if positive else -1)\n",
        "          positive = not positive\n",
        "      labels = np.array(labels, dtype=np.int32)\n",
        "\n",
        "      self._pairs = pairs\n",
        "      self._labels = labels\n",
        "\n",
        "    ptr = 0\n",
        "    while ptr + batch_size <= len(pairs):\n",
        "      batch_graphs = pairs[ptr:ptr + batch_size]\n",
        "      packed_batch = self._pack_batch(batch_graphs)\n",
        "      yield packed_batch, labels[ptr:ptr + batch_size]\n",
        "      ptr += batch_size"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKY5FeT9pu0s"
      },
      "source": [
        "## Building the model, and the training and evaluation pipelines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y5vCmOYjpxFa"
      },
      "source": [
        "### Configs\n",
        "\n",
        "We put all the configs for model building and training into a single `dict`, but any part of our code can also be used separately with separate configs if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "whHJHVMupwjT"
      },
      "outputs": [],
      "source": [
        "def get_default_config():\n",
        "  \"\"\"The default configs.\"\"\"\n",
        "  node_state_dim = 32\n",
        "  graph_rep_dim = 128\n",
        "  graph_embedding_net_config = dict(\n",
        "      node_state_dim=node_state_dim,\n",
        "      edge_hidden_sizes=[node_state_dim * 2, node_state_dim * 2],\n",
        "      node_hidden_sizes=[node_state_dim * 2],\n",
        "      n_prop_layers=5,\n",
        "      # set to False to not share parameters across message passing layers\n",
        "      share_prop_params=True,\n",
        "      # initialize message MLP with small parameter weights to prevent\n",
        "      # aggregated message vectors blowing up, alternatively we could also use\n",
        "      # e.g. layer normalization to keep the scale of these under control.\n",
        "      edge_net_init_scale=0.1,\n",
        "      # other types of update like `mlp` and `residual` can also be used here.\n",
        "      node_update_type='gru',\n",
        "      # set to False if your graph already contains edges in both directions.\n",
        "      use_reverse_direction=True,\n",
        "      # set to True if your graph is directed\n",
        "      reverse_dir_param_different=False,\n",
        "      # we didn't use layer norm in our experiments but sometimes this can help.\n",
        "      layer_norm=False)\n",
        "  graph_matching_net_config = graph_embedding_net_config.copy()\n",
        "  graph_matching_net_config['similarity'] = 'dotproduct'\n",
        "\n",
        "  return dict(\n",
        "      encoder=dict(\n",
        "          node_hidden_sizes=[node_state_dim],\n",
        "          edge_hidden_sizes=None),\n",
        "      aggregator=dict(\n",
        "          node_hidden_sizes=[graph_rep_dim],\n",
        "          graph_transform_sizes=[graph_rep_dim],\n",
        "          gated=True,\n",
        "          aggregation_type='sum'),\n",
        "      graph_embedding_net=graph_embedding_net_config,\n",
        "      graph_matching_net=graph_matching_net_config,\n",
        "      # Set to `embedding` to use the graph embedding net.\n",
        "      model_type='matching',\n",
        "      # DSB Modification: Commented out the configuration for the graph_edit_distance and entered the config for the attributed_graph dataset.\n",
        "      data=dict(\n",
        "        problem=\"attributed_graph\",\n",
        "        dataset_params=dict(\n",
        "            folder=\"/home/giuliadelo/Code/learning-garteur/data\",\n",
        "            dataset=\"learning-garteur-5-scale-groups-250-6-fuselage-3-wing-3-winglet-3-vertical-tail-1-horizontal-tail\",\n",
        "            population_regex=\"-\\w+-winglets\",\n",
        "            validation_percentage = 1,\n",
        "            test_size = 80\n",
        "        )\n",
        "      ),\n",
        "    #   data=dict(\n",
        "    #       problem='graph_edit_distance',\n",
        "    #       dataset_params=dict(\n",
        "    #           # always generate graphs with 20 nodes and p_edge=0.2.\n",
        "    #           n_nodes_range=[20, 20],\n",
        "    #           p_edge_range=[0.2, 0.2],\n",
        "    #           n_changes_positive=1,\n",
        "    #           n_changes_negative=2,\n",
        "    #           validation_dataset_size=1000)),\n",
        "      training=dict(\n",
        "          batch_size=20,\n",
        "          learning_rate=1e-3,\n",
        "          mode='pair',\n",
        "          loss='margin',\n",
        "          margin=1.0,\n",
        "          # A small regularizer on the graph vector scales to avoid the graph\n",
        "          # vectors blowing up.  If numerical issues is particularly bad in the\n",
        "          # model we can add `snt.LayerNorm` to the outputs of each layer, the\n",
        "          # aggregated messages and aggregated node representations to\n",
        "          # keep the network activation scale in a reasonable range.\n",
        "          graph_vec_regularizer_weight=1e-6,\n",
        "          # Add gradient clipping to avoid large gradients.\n",
        "          clip_value=10.0,\n",
        "          # Increase this to train longer.\n",
        "          n_training_steps=500000,\n",
        "          # Print training information every this many training steps.\n",
        "          print_after=100,\n",
        "          # Evaluate on validation set every `eval_after * print_after` steps.\n",
        "          eval_after=5),\n",
        "      evaluation=dict(\n",
        "          batch_size=20),\n",
        "      seed=6,\n",
        "      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIxpwPRFRYE1"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "We evaluate the performance of the models by measuring how well they do on held-out data.  We look at two metrics, triplet accuracy and pair AUC.\n",
        "\n",
        "For each triplet $(G_1, G_2, G_3)$, a model is said to make a correct prediction if it predicts $d(G_1, G_2) < d(G_1, G_3)$, i.e. similar graphs have a smaller distance (or larger similarity) than dissimilar graphs.  We average the accuracy across a dataset to get the **triplet accuracy** metric.\n",
        "\n",
        "For each pair $(G_1, G_2)$ a model computes a distance $d(G_1, G_2)$.  To compute the **pair AUC** metric, we rank all the pairs by the distance from low to high (similarity from high to low), and then compute the area under the ROC curve for positive pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PQkjMzZxpmml"
      },
      "outputs": [],
      "source": [
        "def exact_hamming_similarity(x, y):\n",
        "  \"\"\"Compute the binary Hamming similarity.\"\"\"\n",
        "  match = tf.cast(tf.equal(x > 0, y > 0), dtype=tf.float32)\n",
        "  return tf.reduce_mean(match, axis=1)\n",
        "\n",
        "\n",
        "def compute_similarity(config, x, y):\n",
        "  \"\"\"Compute the distance between x and y vectors.\n",
        "\n",
        "  The distance will be computed based on the training loss type.\n",
        "\n",
        "  Args:\n",
        "    config: a config dict.\n",
        "    x: [n_examples, feature_dim] float tensor.\n",
        "    y: [n_examples, feature_dim] float tensor.\n",
        "\n",
        "  Returns:\n",
        "    dist: [n_examples] float tensor.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if loss type is not supported.\n",
        "  \"\"\"\n",
        "  if config['training']['loss'] == 'margin':\n",
        "    # similarity is negative distance\n",
        "    return -euclidean_distance(x, y)\n",
        "  elif config['training']['loss'] == 'hamming':\n",
        "    return exact_hamming_similarity(x, y)\n",
        "  else:\n",
        "    raise ValueError('Unknown loss type %s' % config['training']['loss'])\n",
        "\n",
        "\n",
        "def auc(scores, labels, **auc_args):\n",
        "  \"\"\"Compute the AUC for pair classification.\n",
        "\n",
        "  See `tf.metrics.auc` for more details about this metric.\n",
        "\n",
        "  Args:\n",
        "    scores: [n_examples] float.  Higher scores mean higher preference of being\n",
        "      assigned the label of +1.\n",
        "    labels: [n_examples] int.  Labels are either +1 or -1.\n",
        "    **auc_args: other arguments that can be used by `tf.metrics.auc`.\n",
        "\n",
        "  Returns:\n",
        "    auc: the area under the ROC curve.\n",
        "  \"\"\"\n",
        "  scores_max = tf.reduce_max(scores)\n",
        "  scores_min = tf.reduce_min(scores)\n",
        "  # normalize scores to [0, 1] and add a small epislon for safety\n",
        "  scores = (scores - scores_min) / (scores_max - scores_min + 1e-8)\n",
        "\n",
        "  labels = (labels + 1) / 2\n",
        "  # The following code should be used according to the tensorflow official\n",
        "  # documentation:\n",
        "  # value, _ = tf.metrics.auc(labels, scores, **auc_args)\n",
        "\n",
        "  # However `tf.metrics.auc` is currently (as of July 23, 2019) buggy so we have\n",
        "  # to use the following:\n",
        "  _, value = tf.metrics.auc(labels, scores, **auc_args)\n",
        "  return value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nB_sczkuzHUD"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "We need to:\n",
        "* set up the placeholders\n",
        "* build the model\n",
        "* build the computation graphs for training and evaluation\n",
        "* build the metrics and statistics to monitor\n",
        "\n",
        "We assume the graphs are batched, if a batch contains pairs $(G_1^1, G_2^1), (G_1^2, G_2^2), ...$, then the graphs will be packed in a batch as a sequence of graphs $(G_1^1, G_2^1, G_1^2, G_2^2, ...)$.  If a batch contains triplets $(G_1^1, G_2^1, G_3^1), (G_1^2, G_2^2, G_3^2)$ then the graphs will be packed in a batch as a sequence of $(G_1^1, G_2^1, G_1^1, G_3^1, G_1^2, G_2^2, G_1^2, G_3^2), ...$.  Note that the first graph in each triplet is duplicated once to make the cross-graph attention more easily computable as it requires the graphs to appear in pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uNBL6Om3zJJb"
      },
      "outputs": [],
      "source": [
        "def reshape_and_split_tensor(tensor, n_splits):\n",
        "  \"\"\"Reshape and split a 2D tensor along the last dimension.\n",
        "\n",
        "  Args:\n",
        "    tensor: a [num_examples, feature_dim] tensor.  num_examples must be a\n",
        "      multiple of `n_splits`.\n",
        "    n_splits: int, number of splits to split the tensor into.\n",
        "\n",
        "  Returns:\n",
        "    splits: a list of `n_splits` tensors.  The first split is [tensor[0],\n",
        "      tensor[n_splits], tensor[n_splits * 2], ...], the second split is\n",
        "      [tensor[1], tensor[n_splits + 1], tensor[n_splits * 2 + 1], ...], etc..\n",
        "  \"\"\"\n",
        "  feature_dim = tensor.shape.as_list()[-1]\n",
        "  # feature dim must be known, otherwise you can provide that as an input\n",
        "  assert isinstance(feature_dim, int)\n",
        "  tensor = tf.reshape(tensor, [-1, feature_dim * n_splits])\n",
        "  return tf.split(tensor, n_splits, axis=-1)\n",
        "\n",
        "\n",
        "def build_placeholders(node_feature_dim, edge_feature_dim):\n",
        "  \"\"\"Build the placeholders needed for the model.\n",
        "\n",
        "  Args:\n",
        "    node_feature_dim: int.\n",
        "    edge_feature_dim: int.\n",
        "\n",
        "  Returns:\n",
        "    placeholders: a placeholder name -> placeholder tensor dict.\n",
        "  \"\"\"\n",
        "  # `n_graphs` must be specified as an integer, as `tf.dynamic_partition`\n",
        "  # requires so.\n",
        "  return {\n",
        "      'node_features': tf.placeholder(tf.float32, [None, node_feature_dim]),\n",
        "      'edge_features': tf.placeholder(tf.float32, [None, edge_feature_dim]),\n",
        "      'from_idx': tf.placeholder(tf.int32, [None]),\n",
        "      'to_idx': tf.placeholder(tf.int32, [None]),\n",
        "      'graph_idx': tf.placeholder(tf.int32, [None]),\n",
        "      # only used for pairwise training and evaluation\n",
        "      'labels': tf.placeholder(tf.int32, [None]),\n",
        "  }\n",
        "\n",
        "\n",
        "def build_model(config, node_feature_dim, edge_feature_dim):\n",
        "  \"\"\"Create model for training and evaluation.\n",
        "\n",
        "  Args:\n",
        "    config: a dictionary of configs, like the one created by the\n",
        "      `get_default_config` function.\n",
        "    node_feature_dim: int, dimensionality of node features.\n",
        "    edge_feature_dim: int, dimensionality of edge features.\n",
        "\n",
        "  Returns:\n",
        "    tensors: a (potentially nested) name => tensor dict.\n",
        "    placeholders: a (potentially nested) name => tensor dict.\n",
        "    model: a GraphEmbeddingNet or GraphMatchingNet instance.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if the specified model or training settings are not supported.\n",
        "  \"\"\"\n",
        "  encoder = GraphEncoder(**config['encoder'])\n",
        "  aggregator = GraphAggregator(**config['aggregator'])\n",
        "  if config['model_type'] == 'embedding':\n",
        "    model = GraphEmbeddingNet(\n",
        "        encoder, aggregator, **config['graph_embedding_net'])\n",
        "  elif config['model_type'] == 'matching':\n",
        "    model = GraphMatchingNet(\n",
        "        encoder, aggregator, **config['graph_matching_net'])\n",
        "  else:\n",
        "    raise ValueError('Unknown model type: %s' % config['model_type'])\n",
        "\n",
        "  training_n_graphs_in_batch = config['training']['batch_size']\n",
        "  if config['training']['mode'] == 'pair':\n",
        "    training_n_graphs_in_batch *= 2\n",
        "  elif config['training']['mode'] == 'triplet':\n",
        "    training_n_graphs_in_batch *= 4\n",
        "  else:\n",
        "    raise ValueError('Unknown training mode: %s' % config['training']['mode'])\n",
        "\n",
        "  placeholders = build_placeholders(node_feature_dim, edge_feature_dim)\n",
        "\n",
        "  # training\n",
        "  model_inputs = placeholders.copy()\n",
        "  del model_inputs['labels']\n",
        "  model_inputs['n_graphs'] = training_n_graphs_in_batch\n",
        "  graph_vectors = model(**model_inputs)\n",
        "\n",
        "  if config['training']['mode'] == 'pair':\n",
        "    x, y = reshape_and_split_tensor(graph_vectors, 2)\n",
        "    loss = pairwise_loss(x, y, placeholders['labels'],\n",
        "                         loss_type=config['training']['loss'],\n",
        "                         margin=config['training']['margin'])\n",
        "\n",
        "    # optionally monitor the similarity between positive and negative pairs\n",
        "    is_pos = tf.cast(tf.equal(placeholders['labels'], 1), tf.float32)\n",
        "    is_neg = 1 - is_pos\n",
        "    n_pos = tf.reduce_sum(is_pos)\n",
        "    n_neg = tf.reduce_sum(is_neg)\n",
        "    sim = compute_similarity(config, x, y)\n",
        "    sim_pos = tf.reduce_sum(sim * is_pos) / (n_pos + 1e-8)\n",
        "    sim_neg = tf.reduce_sum(sim * is_neg) / (n_neg + 1e-8)\n",
        "  else:\n",
        "    x_1, y, x_2, z = reshape_and_split_tensor(graph_vectors, 4)\n",
        "    loss = triplet_loss(x_1, y, x_2, z,\n",
        "                        loss_type=config['training']['loss'],\n",
        "                        margin=config['training']['margin'])\n",
        "\n",
        "    sim_pos = tf.reduce_mean(compute_similarity(config, x_1, y))\n",
        "    sim_neg = tf.reduce_mean(compute_similarity(config, x_2, z))\n",
        "\n",
        "  graph_vec_scale = tf.reduce_mean(graph_vectors**2)\n",
        "  if config['training']['graph_vec_regularizer_weight'] > 0:\n",
        "    loss += (config['training']['graph_vec_regularizer_weight'] *\n",
        "             0.5 * graph_vec_scale)\n",
        "\n",
        "  # monitor scale of the parameters and gradients, these are typically helpful\n",
        "  optimizer = tf.train.AdamOptimizer(\n",
        "      learning_rate=config['training']['learning_rate'])\n",
        "  grads_and_params = optimizer.compute_gradients(loss)\n",
        "  grads, params = zip(*grads_and_params)\n",
        "  grads, _ = tf.clip_by_global_norm(grads, config['training']['clip_value'])\n",
        "  train_step = optimizer.apply_gradients(zip(grads, params))\n",
        "\n",
        "  grad_scale = tf.global_norm(grads)\n",
        "  param_scale = tf.global_norm(params)\n",
        "\n",
        "  # evaluation\n",
        "  model_inputs['n_graphs'] = config['evaluation']['batch_size'] * 2\n",
        "  eval_pairs = model(**model_inputs)\n",
        "  x, y = reshape_and_split_tensor(eval_pairs, 2)\n",
        "  similarity = compute_similarity(config, x, y)\n",
        "  pair_auc = auc(similarity, placeholders['labels'])\n",
        "\n",
        "  model_inputs['n_graphs'] = config['evaluation']['batch_size'] * 4\n",
        "  eval_triplets = model(**model_inputs)\n",
        "  x_1, y, x_2, z = reshape_and_split_tensor(eval_triplets, 4)\n",
        "  sim_1 = compute_similarity(config, x_1, y)\n",
        "  sim_2 = compute_similarity(config, x_2, z)\n",
        "  triplet_acc = tf.reduce_mean(tf.cast(sim_1 > sim_2, dtype=tf.float32))\n",
        "\n",
        "  return {\n",
        "      'train_step': train_step,\n",
        "      'metrics': {\n",
        "          'training': {\n",
        "              'loss': loss,\n",
        "              'grad_scale': grad_scale,\n",
        "              'param_scale': param_scale,\n",
        "              'graph_vec_scale': graph_vec_scale,\n",
        "              'sim_pos': sim_pos,\n",
        "              'sim_neg': sim_neg,\n",
        "              'sim_diff': sim_pos - sim_neg,\n",
        "          },\n",
        "          'validation': {\n",
        "              'pair_sim': similarity,\n",
        "              'pair_auc': pair_auc,\n",
        "              'triplet_acc': triplet_acc,\n",
        "          },\n",
        "      },\n",
        "  }, placeholders, model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "09KlUpfZt-L9"
      },
      "source": [
        "### The training pipeline\n",
        "\n",
        "For this we need to build the datasets, handle `feed_dict`s and run some evaluation during training.\n",
        "\n",
        "Note that this training pipeline is only supposed to be used as an example and you may want to add your own checkpointing or experiment monitoring tools (e.g. tensorboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HRGXuCBsucKH"
      },
      "outputs": [],
      "source": [
        "def build_datasets(config):\n",
        "  \"\"\"Build the training and evaluation datasets.\"\"\"\n",
        "  config = copy.deepcopy(config)\n",
        "\n",
        "  if config['data']['problem'] == 'graph_edit_distance':\n",
        "    dataset_params = config['data']['dataset_params']\n",
        "    validation_dataset_size = dataset_params['validation_dataset_size']\n",
        "    del dataset_params['validation_dataset_size']\n",
        "    training_set = GraphEditDistanceDataset(**dataset_params)\n",
        "    dataset_params['dataset_size'] = validation_dataset_size\n",
        "    validation_set = FixedGraphEditDistanceDataset(**dataset_params)\n",
        "  elif config['data']['problem'] == \"attributed_graph\":\n",
        "    # DSB Modification: included the option to load the attributed graph problem\n",
        "    training_set, validation_set, test_set = AttributedGraphDataset.load_dataset(\n",
        "      config['data']['dataset_params'][\"folder\"],\n",
        "      config['data']['dataset_params'][\"dataset\"],\n",
        "      config['data']['dataset_params'][\"population_regex\"],\n",
        "      config['data']['dataset_params'][\"validation_percentage\"],\n",
        "      config['data']['dataset_params'][\"test_size\"],\n",
        "      config['evaluation']['batch_size']\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError('Unknown problem type: %s' % config['data']['problem'])\n",
        "  return training_set, validation_set, test_set\n",
        "\n",
        "\n",
        "def fill_feed_dict(placeholders, batch):\n",
        "  \"\"\"Create a feed dict for the given batch of data.\n",
        "\n",
        "  Args:\n",
        "    placeholders: a dict of placeholders.\n",
        "    batch: a batch of data, should be either a single `GraphData` instance for\n",
        "      triplet training, or a tuple of (graphs, labels) for pairwise training.\n",
        "\n",
        "  Returns:\n",
        "    feed_dict: a feed_dict that can be used in a session run call.\n",
        "  \"\"\"\n",
        "  if isinstance(batch, GraphData):\n",
        "    graphs = batch\n",
        "    labels = None\n",
        "  else:\n",
        "    graphs, labels = batch\n",
        "\n",
        "  feed_dict = {\n",
        "      placeholders['node_features']: graphs.node_features,\n",
        "      placeholders['edge_features']: graphs.edge_features,\n",
        "      placeholders['from_idx']: graphs.from_idx,\n",
        "      placeholders['to_idx']: graphs.to_idx,\n",
        "      placeholders['graph_idx']: graphs.graph_idx,\n",
        "  }\n",
        "  if labels is not None:\n",
        "    feed_dict[placeholders['labels']] = labels\n",
        "  return feed_dict\n",
        "\n",
        "\n",
        "def evaluate(sess, eval_metrics, placeholders, validation_set, batch_size):\n",
        "  \"\"\"Evaluate model performance on the given validation set.\n",
        "\n",
        "  Args:\n",
        "    sess: a `tf.Session` instance used to run the computation.\n",
        "    eval_metrics: a dict containing two tensors 'pair_auc' and 'triplet_acc'.\n",
        "    placeholders: a placeholder dict.\n",
        "    validation_set: a `GraphSimilarityDataset` instance, calling `pairs` and\n",
        "      `triplets` functions with `batch_size` creates iterators over a finite\n",
        "      sequence of batches to evaluate on.\n",
        "    batch_size: number of batches to use for each session run call.\n",
        "\n",
        "  Returns:\n",
        "    metrics: a dict of metric name => value mapping.\n",
        "  \"\"\"\n",
        "  accumulated_pair_auc = []\n",
        "  for batch in validation_set.pairs(batch_size):\n",
        "    feed_dict = fill_feed_dict(placeholders, batch)\n",
        "    pair_auc = sess.run(eval_metrics['pair_auc'], feed_dict=feed_dict)\n",
        "    accumulated_pair_auc.append(pair_auc)\n",
        "\n",
        "  # DSB Modification: Commented out the triplet evaluation as the AttributedGraphDataset doesn't support this\n",
        "  # accumulated_triplet_acc = []\n",
        "  # for batch in validation_set.triplets(batch_size):\n",
        "  #   feed_dict = fill_feed_dict(placeholders, batch)\n",
        "  #   triplet_acc = sess.run(eval_metrics['triplet_acc'], feed_dict=feed_dict)\n",
        "  #   accumulated_triplet_acc.append(triplet_acc)\n",
        "\n",
        "  return {\n",
        "      'pair_auc': np.mean(accumulated_pair_auc),\n",
        "      'triplet_acc': 0,\n",
        "  }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-u2iAaBujIg"
      },
      "source": [
        "### Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "65drio8Qu9qG"
      },
      "outputs": [],
      "source": [
        "config = get_default_config()\n",
        "\n",
        "# Let's just run for a small number of training steps.  This may take you a few\n",
        "# minutes.\n",
        "config['training']['n_training_steps'] = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 887
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 590938,
          "status": "ok",
          "timestamp": 1563980116419,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "v7Dz1haMuh8O",
        "outputId": "6722538e-db5a-4538-dc2c-f08108a1b983"
      },
      "outputs": [],
      "source": [
        "# Run this if you want to run the code again, otherwise tensorflow would\n",
        "# complain that you already created the same graph and the same variables.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Set random seeds\n",
        "seed = config['seed']\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "tf.set_random_seed(seed + 2)\n",
        "\n",
        "training_set, validation_set, test_set = build_datasets(config)\n",
        "\n",
        "if config['training']['mode'] == 'pair':\n",
        "  training_data_iter = training_set.pairs(config['training']['batch_size'])\n",
        "  first_batch_graphs, _ = next(training_data_iter)\n",
        "else:\n",
        "  training_data_iter = training_set.triplets(config['training']['batch_size'])\n",
        "  first_batch_graphs = next(training_data_iter)\n",
        "\n",
        "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
        "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
        "\n",
        "tensors, placeholders, model = build_model(\n",
        "    config, node_feature_dim, edge_feature_dim)\n",
        "\n",
        "accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "init_ops = (tf.global_variables_initializer(),\n",
        "            tf.local_variables_initializer())\n",
        "\n",
        "# If we already have a session instance, close it and start a new one\n",
        "if 'sess' in globals():\n",
        "  sess.close()\n",
        "\n",
        "# We will need to keep this session instance around for e.g. visualization.\n",
        "# But you should probably wrap it in a `with tf.Session() sess:` context if you\n",
        "# want to use the code elsewhere.\n",
        "sess = tf.Session()\n",
        "sess.run(init_ops)\n",
        "\n",
        "# use xrange here if you are still on python 2\n",
        "for i_iter in range(config['training']['n_training_steps']):\n",
        "  batch = next(training_data_iter)\n",
        "  _, train_metrics = sess.run(\n",
        "      [tensors['train_step'], tensors['metrics']['training']],\n",
        "      feed_dict=fill_feed_dict(placeholders, batch))\n",
        "\n",
        "  # accumulate over minibatches to reduce variance in the training metrics\n",
        "  for k, v in train_metrics.items():\n",
        "    accumulated_metrics[k].append(v)\n",
        "\n",
        "  if (i_iter + 1) % config['training']['print_after'] == 0:\n",
        "    metrics_to_print = {\n",
        "        k: np.mean(v) for k, v in accumulated_metrics.items()}\n",
        "    info_str = ', '.join(\n",
        "        ['%s %.8f' % (k, v) for k, v in metrics_to_print.items()])\n",
        "    # reset the metrics\n",
        "    accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "    if ((i_iter + 1) // config['training']['print_after'] %\n",
        "        config['training']['eval_after'] == 0):\n",
        "      eval_metrics = evaluate(\n",
        "          sess, tensors['metrics']['validation'], placeholders,\n",
        "          validation_set, config['evaluation']['batch_size'])\n",
        "      info_str += ', ' + ', '.join(\n",
        "          ['%s %.4f' % ('val/' + k, v) for k, v in eval_metrics.items()])\n",
        "\n",
        "    print('iter %d, %s, time %.2fs' % (\n",
        "        i_iter + 1, info_str, time.time() - t_start))\n",
        "    t_start = time.time()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EDHrQOOYzH2t"
      },
      "source": [
        "*#DSB Modification*\n",
        "\n",
        "Note that albeit a bit noisy, the loss is going down, the difference between the similar and disimilar results are going up and the pair AUC is around 0.98."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NqaY9In2zKUT"
      },
      "source": [
        "## Test the model \n",
        "*# DSB Modification*\n",
        "\n",
        "Once the model is trained, we can test on our unseed test set. We generate the similarity score for all of this data and put it into a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Similarity\n",
        "similarity_metrics = []\n",
        "for batch in test_set.pairs(config['evaluation']['batch_size']):\n",
        "    feed_dict = fill_feed_dict(placeholders, batch)\n",
        "    pair_similarity = sess.run(tensors['metrics']['validation']['pair_sim'], feed_dict=feed_dict)\n",
        "    for similarity in range(0, len(pair_similarity)):\n",
        "        similarity_metrics.append(pair_similarity[similarity])\n",
        "        \n",
        "\n",
        "# Scale Similarity\n",
        "similarity_max = np.max(similarity_metrics)\n",
        "similarity_min = np.min(similarity_metrics)\n",
        "scaled_similarity_metrics = [(x - similarity_max) / (similarity_min - similarity_max)  for x in similarity_metrics]\n",
        "     \n",
        "\n",
        "# Order Aeroplane IDs\n",
        "sorted_population_list = sorted(list(dict.fromkeys(test_set._populations)))\n",
        "sorted_id_list = []\n",
        "for population in sorted_population_list:\n",
        "    id_list = []\n",
        "    for i in range(0, len(test_set._populations)):\n",
        "        if test_set._populations[i] != population:\n",
        "            continue\n",
        "        id_list.append(test_set._ids[i])\n",
        "    sorted_id_list.extend(sorted(id_list))\n",
        "\n",
        "# Similarity Matrix\n",
        "id_similarity_matrix = np.zeros((len(sorted_id_list), len(sorted_id_list)))\n",
        "for i in range(0, len(scaled_similarity_metrics)):\n",
        "    id_similarity_matrix[sorted_id_list.index(test_set._names[i][0])][sorted_id_list.index(test_set._names[i][1])] = scaled_similarity_metrics[i]\n",
        "\n",
        "# Generate Friendly Names\n",
        "sorted_id_friendly_list = []\n",
        "for i in range(0, len(sorted_id_list)):\n",
        "    match = re.search(rf\"\\d+-\\w+-winglets-scale-group-+\\d\", sorted_id_list[i])\n",
        "    parts = match.group().split(\"-\")\n",
        "    sorted_id_friendly_list.append(\"{0}-s{1} (#{2})\".format(parts[1], parts[-1], parts[0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This matrix is then displayed as a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Display Heat Map\n",
        "fig, ax = plt.subplots(figsize=(10,10), dpi=300)\n",
        "im = ax.imshow(id_similarity_matrix)\n",
        "\n",
        "ax.set_xticks(np.arange(len(test_set._ids)), labels=sorted_id_friendly_list, fontsize=\"4\", ha=\"right\", rotation=45, rotation_mode=\"anchor\")\n",
        "ax.set_yticks(np.arange(len(test_set._ids)), labels=sorted_id_friendly_list, fontsize=\"4\", ha=\"left\", va=\"center\", position=(-0.05, 0))\n",
        "\n",
        "for i in range(len(sorted_id_list)):\n",
        "    for j in range(len(sorted_id_list)):\n",
        "        text = ax.text(j, i, \"{:.2f}\".format(id_similarity_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\", fontsize=\"4\")\n",
        "\n",
        "# fig.colorbar(im)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paper Outputs \n",
        "*# DSB Modification*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the desired input\n",
        "desired_inputs = {}\n",
        "desired_populations = []\n",
        "for i,input in enumerate(sorted_id_friendly_list):\n",
        "    parts = input.split(\" \")\n",
        "    parts2 = input.split(\"-\")\n",
        "\n",
        "    if parts[0] not in desired_populations and (parts2[1][1] in {\"1\",\"3\",\"5\"})  :\n",
        "    #if parts[0] not in desired_populations:\n",
        "        desired_inputs[input] = i\n",
        "        desired_populations.append(parts[0])\n",
        "\n",
        "#Get Matrix\n",
        "desired_id_matrix = np.zeros((len(desired_inputs), len(desired_inputs)))\n",
        "for i, ikey in enumerate(desired_inputs):\n",
        "    for j, jkey in enumerate(desired_inputs):\n",
        "        desired_id_matrix[i][j] = id_similarity_matrix[desired_inputs[ikey]][desired_inputs[jkey]]\n",
        "\n",
        "#Display Heat Map\n",
        "fig, ax = plt.subplots(figsize=(2.5,2.5), dpi=300)\n",
        "im = ax.imshow(desired_id_matrix)\n",
        "\n",
        "ax.set_xticks(np.arange(len(desired_inputs)), labels=desired_inputs, fontsize=\"3\", ha=\"right\", rotation=45, rotation_mode=\"anchor\")\n",
        "ax.set_yticks(np.arange(len(desired_inputs)), labels=desired_inputs.keys(), ha=\"left\", va=\"center\", position=(-0.2, 0), fontsize=\"3\")\n",
        "\n",
        "for i in range(len(desired_inputs)):\n",
        "    for j in range(len(desired_inputs)):\n",
        "        text = ax.text(j, i, \"{:.2f}\".format(desired_id_matrix[i, j] ), ha=\"center\", va=\"center\", color=\"w\", fontsize=\"3\")\n",
        "\n",
        "# fig.colorbar(im)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HrOZEU_OrYTR"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "graph_matching_networks.ipynb",
      "provenance": [
        {
          "file_id": "1Ys7zuxnbz-5AbNTMmC5Rf4rEOXXvsVEV",
          "timestamp": 1563976318651
        }
      ],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3.7.15 ('.env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "e1fbbd4ff3cbfe8b4d48c2d952ff3ff876613e1c43df03f26d03c8c2a6595313"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
